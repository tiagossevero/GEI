{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70aea681-2477-45f5-9ffe-1cb8f6cb1a68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'.env_file' loaded!\n",
      "ENV 'PROD' configured!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/tsevero/notebooks/SAT_BIG_DATA/data-pipeline/batch/poc\")\n",
    "sys.path.append(\"/home/tsevero/notebooks/SAT_BIG_DATA/data-pipeline/batch/plugins\")\n",
    "sys.path.append(\"/home/tsevero/notebooks/SAT_BIG_DATA/data-pipeline/batch/dags\")\n",
    "\n",
    "#Import libs python\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import date\n",
    "\n",
    "#Import libs internas\n",
    "from utils import spark_utils_session as utils\n",
    "\n",
    "from hooks.hdfs.hdfs_helper import HdfsHelper\n",
    "from jobs.job_base_config import BaseETLJobClass\n",
    "\n",
    "import poc_helper\n",
    "poc_helper.load_env(\"PROD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4faf8a1f-48e4-4c57-a162-c839c8e3a940",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-10-24T21:35:35.292556Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mUsing json file settings.     \u001b[0m [\u001b[0m\u001b[1m\u001b[34mroot\u001b[0m]\u001b[0m \u001b[36mloc\u001b[0m=\u001b[35mspark_utils_session.py:301\u001b[0m\n",
      "\u001b[2m2025-10-24T21:35:35.293793Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mExporting default ENV.        \u001b[0m [\u001b[0m\u001b[1m\u001b[34mroot\u001b[0m]\u001b[0m \u001b[36mloc\u001b[0m=\u001b[35mspark_utils_session.py:305\u001b[0m\n",
      "\u001b[2m2025-10-24T21:35:35.294177Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mExporting custom ENVs.        \u001b[0m [\u001b[0m\u001b[1m\u001b[34mroot\u001b[0m]\u001b[0m \u001b[36mloc\u001b[0m=\u001b[35mspark_utils_session.py:338\u001b[0m\n",
      "\u001b[2m2025-10-24T21:35:35.294631Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mBuilding profile 'efd_t2'.    \u001b[0m [\u001b[0m\u001b[1m\u001b[34mroot\u001b[0m]\u001b[0m \u001b[36mloc\u001b[0m=\u001b[35mspark_utils_session.py:221\u001b[0m\n",
      "\u001b[2m2025-10-24T21:35:35.294993Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNot enough info for building the kerberos client. Ignoring it\u001b[0m [\u001b[0m\u001b[1m\u001b[34mroot\u001b[0m]\u001b[0m \u001b[36mloc\u001b[0m=\u001b[35mspark_utils_session.py:284\u001b[0m\n",
      "Warning: Ignoring non-Spark config property: hive.metastore.uris\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/cloudera/parcels/SPARK3-3.5.4.3.5.7191000.0-30-1.p0.68499982/lib/spark3/jars/ivy-2.5.2.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/tsevero/.ivy2/cache\n",
      "The jars for the packages stored in: /home/tsevero/.ivy2/jars\n",
      "com.databricks#spark-xml_2.12 added as a dependency\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "com.oracle.database.security#oraclepki added as a dependency\n",
      "com.oracle.database.security#osdt_core added as a dependency\n",
      "com.oracle.database.security#osdt_cert added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-6e2d8bbc-b495-48e5-8b8e-c644532149a0;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.databricks#spark-xml_2.12;0.18.0 in central\n",
      "\tfound commons-io#commons-io;2.11.0 in central\n",
      "\tfound org.glassfish.jaxb#txw2;3.0.2 in central\n",
      "\tfound org.apache.ws.xmlschema#xmlschema-core;2.3.0 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.9.0 in central\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.9.2 in central\n",
      "\tfound com.oracle.database.security#oraclepki;21.18.0.0 in central\n",
      "\tfound com.oracle.database.security#osdt_core;21.18.0.0 in central\n",
      "\tfound com.oracle.database.security#osdt_cert;21.18.0.0 in central\n",
      ":: resolution report :: resolve 174ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\tcom.databricks#spark-xml_2.12;0.18.0 from central in [default]\n",
      "\tcom.oracle.database.security#oraclepki;21.18.0.0 from central in [default]\n",
      "\tcom.oracle.database.security#osdt_cert;21.18.0.0 from central in [default]\n",
      "\tcom.oracle.database.security#osdt_core;21.18.0.0 from central in [default]\n",
      "\tcommons-io#commons-io;2.11.0 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.9.2 from central in [default]\n",
      "\torg.apache.ws.xmlschema#xmlschema-core;2.3.0 from central in [default]\n",
      "\torg.glassfish.jaxb#txw2;3.0.2 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.9.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   0   |   0   |   0   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-6e2d8bbc-b495-48e5-8b8e-c644532149a0\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/5ms)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/24 18:35:40 WARN  conf.HiveConf: [Thread-9]: HiveConf of name hive.metastore.runworker.in does not exist\n",
      "25/10/24 18:35:40 WARN  conf.HiveConf: [Thread-9]: HiveConf of name hive.masking.algo does not exist\n",
      "25/10/24 18:35:41 WARN  util.Utils: [Thread-9]: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.\n",
      "25/10/24 18:35:41 WARN  yarn.Client: [Thread-9]: Same path resource file:///home/tsevero/.ivy2/jars/com.databricks_spark-xml_2.12-0.18.0.jar added multiple times to distributed cache.\n",
      "25/10/24 18:35:41 WARN  yarn.Client: [Thread-9]: Same path resource file:///home/tsevero/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.9.2.jar added multiple times to distributed cache.\n",
      "25/10/24 18:35:41 WARN  yarn.Client: [Thread-9]: Same path resource file:///home/tsevero/.ivy2/jars/com.oracle.database.security_oraclepki-21.18.0.0.jar added multiple times to distributed cache.\n",
      "25/10/24 18:35:41 WARN  yarn.Client: [Thread-9]: Same path resource file:///home/tsevero/.ivy2/jars/com.oracle.database.security_osdt_core-21.18.0.0.jar added multiple times to distributed cache.\n",
      "25/10/24 18:35:41 WARN  yarn.Client: [Thread-9]: Same path resource file:///home/tsevero/.ivy2/jars/com.oracle.database.security_osdt_cert-21.18.0.0.jar added multiple times to distributed cache.\n",
      "25/10/24 18:35:41 WARN  yarn.Client: [Thread-9]: Same path resource file:///home/tsevero/.ivy2/jars/commons-io_commons-io-2.11.0.jar added multiple times to distributed cache.\n",
      "25/10/24 18:35:41 WARN  yarn.Client: [Thread-9]: Same path resource file:///home/tsevero/.ivy2/jars/org.glassfish.jaxb_txw2-3.0.2.jar added multiple times to distributed cache.\n",
      "25/10/24 18:35:41 WARN  yarn.Client: [Thread-9]: Same path resource file:///home/tsevero/.ivy2/jars/org.apache.ws.xmlschema_xmlschema-core-2.3.0.jar added multiple times to distributed cache.\n",
      "25/10/24 18:35:41 WARN  yarn.Client: [Thread-9]: Same path resource file:///home/tsevero/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.9.0.jar added multiple times to distributed cache.\n",
      "25/10/24 18:35:47 WARN  util.Utils: [Thread-9]: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.\n"
     ]
    }
   ],
   "source": [
    "def get_session(profile: str, dynamic_allocation_enabled: bool = True) -> utils.DBASparkAppSession:\n",
    "    \"\"\"Generates DBASparkAppSession.\"\"\"\n",
    "    \n",
    "    app_name = \"tsevero_gei_calculo\"\n",
    "    \n",
    "    spark_builder = (utils.DBASparkAppSession\n",
    "                     .builder\n",
    "                     .setAppName(app_name)\n",
    "                     .usingProcessProfile(profile)\n",
    "                    )\n",
    "    \n",
    "    if dynamic_allocation_enabled:\n",
    "        spark_builder.autoResourceManagement()\n",
    "\n",
    "    return spark_builder.build()\n",
    "\n",
    "session = get_session(profile='efd_t2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00777b24-69dc-4a6e-ba6f-4e11cd534937",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hive Session ID = e04d7a30-1aa0-4e44-a524-718f8f84378a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|namespace         |\n",
      "+------------------+\n",
      "|anac              |\n",
      "|bcadastro         |\n",
      "|bpe               |\n",
      "|c115              |\n",
      "|ccc               |\n",
      "|ccg               |\n",
      "|cte               |\n",
      "|default           |\n",
      "|destda            |\n",
      "|detran_share      |\n",
      "|dime              |\n",
      "|due               |\n",
      "|efd               |\n",
      "|fci               |\n",
      "|gecob             |\n",
      "|gescol            |\n",
      "|gessimples        |\n",
      "|gplam             |\n",
      "|information_schema|\n",
      "|malhas            |\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session.sparkSession.sql(\"SHOW DATABASES\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "488d8bf8-a229-46d4-8c06-850c68566574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SISTEMA DE ANÃLISE FISCAL - PROCESSAMENTO NFe/CTe\n",
      "================================================================================\n",
      "Iniciado em: 2025-10-24 18:36:03\n",
      "\n",
      "Spark UI: None\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Bibliotecas para anÃ¡lise e visualizaÃ§Ã£o\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime, date\n",
    "from decimal import Decimal\n",
    "\n",
    "# ConfiguraÃ§Ãµes de visualizaÃ§Ã£o\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.rcParams['figure.figsize'] = (16, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Acesso ao SparkSession\n",
    "spark = session.sparkSession\n",
    "\n",
    "# ===================================================================\n",
    "# âœ… IMPORTAÃ‡Ã•ES \"PADRÃƒO OURO\"\n",
    "# ===================================================================\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, trim, regexp_replace, coalesce, \n",
    "    concat, lpad, floor,\n",
    "    sum as spark_sum, \n",
    "    count as spark_count, \n",
    "    avg as spark_avg,\n",
    "    when as spark_when, \n",
    "    desc as spark_desc,\n",
    "    row_number\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "# ===================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SISTEMA DE ANÃLISE FISCAL - PROCESSAMENTO NFe/CTe\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Iniciado em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dcb6645-d9a0-4600-8a23-8d6fd4bb330e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PARÃ‚METROS DA ANÃLISE\n",
      "================================================================================\n",
      "PerÃ­odo: 202001 atÃ© 202509\n",
      "Tabela Cadastro: usr_sat_ods.vw_ods_contrib\n",
      "CNPJs especÃ­ficos: 2\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# CÃ‰LULA 3: PARÃ‚METROS DE ENTRADA\n",
    "# ===================================================================\n",
    "# DEFINA AQUI OS PARÃ‚METROS DA ANÃLISE\n",
    "\n",
    "# PerÃ­odo de anÃ¡lise (formato YYYYMM)\n",
    "PERIODO_INICIO = 202001  # Janeiro/2020\n",
    "PERIODO_FIM = 202509     # Setembro/2025\n",
    "\n",
    "# Lista de CNPJs a serem analisados (cadastro)\n",
    "# Se vazio, busca da tabela de cadastro\n",
    "CNPJS_ANALISE = ['32372396000194', '46909678000192']  # Exemplo: ['12345678000190', '98765432000110']\n",
    "\n",
    "# Se quiser buscar de uma tabela especÃ­fica:\n",
    "TABELA_CADASTRO = \"usr_sat_ods.vw_ods_contrib\"  # Ajuste conforme sua tabela\n",
    "CAMPO_CNPJ_CADASTRO = \"nu_cnpj\"  # Campo que contÃ©m o CNPJ\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PARÃ‚METROS DA ANÃLISE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"PerÃ­odo: {PERIODO_INICIO} atÃ© {PERIODO_FIM}\")\n",
    "print(f\"Tabela Cadastro: {TABELA_CADASTRO}\")\n",
    "print(f\"CNPJs especÃ­ficos: {len(CNPJS_ANALISE) if CNPJS_ANALISE else 'Todos da tabela de cadastro'}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a17742d-e289-45f6-9fcc-d2fc7b14f012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CARREGANDO CNPJS DO CADASTRO\n",
      "================================================================================\n",
      "âœ“ Usando 2 CNPJs fornecidos manualmente\n",
      "\n",
      "Cacheando tabela 'cadastro_cnpj'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Tabela 'cadastro_cnpj' cacheada com 2 registros.\n",
      "\n",
      "Amostra de CNPJs:\n",
      "+--------------+\n",
      "|cnpj          |\n",
      "+--------------+\n",
      "|32372396000194|\n",
      "|46909678000192|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# CÃ‰LULA 4: CARREGAR CNPJS DO CADASTRO (Refatorado - API)\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CARREGANDO CNPJS DO CADASTRO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if CNPJS_ANALISE:\n",
    "    # Usar lista manual\n",
    "    df_cadastro = spark.createDataFrame(\n",
    "        [(cnpj,) for cnpj in CNPJS_ANALISE], \n",
    "        [\"cnpj\"]\n",
    "    )\n",
    "    print(f\"âœ“ Usando {len(CNPJS_ANALISE)} CNPJs fornecidos manualmente\")\n",
    "else:\n",
    "    # Buscar da tabela usando API de DataFrame\n",
    "    df_cadastro = (spark.table(TABELA_CADASTRO)\n",
    "        .select(\n",
    "            regexp_replace(\n",
    "                trim(col(CAMPO_CNPJ_CADASTRO).cast(\"string\")), \n",
    "                '[^0-9]', ''\n",
    "            ).alias(\"cnpj\")\n",
    "        )\n",
    "        .where(col(\"cnpj\").isNotNull())\n",
    "        .distinct()\n",
    "    )\n",
    "    \n",
    "    total_cnpjs = df_cadastro.count()\n",
    "    print(f\"âœ“ Carregados {total_cnpjs} CNPJs da tabela {TABELA_CADASTRO}\")\n",
    "\n",
    "# Criar view temporÃ¡ria\n",
    "df_cadastro.createOrReplaceTempView(\"cadastro_cnpj\")\n",
    "\n",
    "# Materializar os CNPJs em cache (MUITO IMPORTANTE para os joins)\n",
    "print(\"\\nCacheando tabela 'cadastro_cnpj'...\")\n",
    "spark.sql(\"CACHE TABLE cadastro_cnpj\")\n",
    "total_cnpjs_cache = spark.table(\"cadastro_cnpj\").count()\n",
    "print(f\"âœ“ Tabela 'cadastro_cnpj' cacheada com {total_cnpjs_cache} registros.\")\n",
    "\n",
    "# Exibir amostra\n",
    "print(\"\\nAmostra de CNPJs:\")\n",
    "spark.table(\"cadastro_cnpj\").limit(10).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cc71eff-8614-4adf-aec9-a9e1b2eff84d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CRIANDO VIEW DE NFe PROCESSADA (OTIMIZADA)\n",
      "================================================================================\n",
      "Fazendo REFRESH da tabela nfe.nfe...\n",
      "âœ“ REFRESH executado\n",
      "CNPJs no cadastro: 2\n",
      "\n",
      "âš ï¸ ATENÃ‡ÃƒO: Pulando perÃ­odo 2020 devido a arquivos corrompidos\n",
      "PerÃ­odo ajustado: 202101 atÃ© 202509\n",
      "âœ“ View criada\n",
      "\n",
      "âœ“ View NFe criada com sucesso\n",
      "PerÃ­odo: 202101 atÃ© 202509\n",
      "\n",
      "Amostra dos dados (5 registros):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:======================================================>(99 + 1) / 100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+-----------------+-------------+-------------+\n",
      "|periodo_ref|cnpj_emitente |cnpj_destinatario|entrada_saida|valor_produto|\n",
      "+-----------+--------------+-----------------+-------------+-------------+\n",
      "|202101     |32372396000194|NULL             |Saida        |7670.00      |\n",
      "|202101     |18563423000185|32372396000194   |Saida        |37498.65     |\n",
      "|202101     |32372396000194|NULL             |Saida        |1770.00      |\n",
      "|202101     |32372396000194|NULL             |Saida        |10192.00     |\n",
      "|202101     |32372396000194|NULL             |Saida        |2565.00      |\n",
      "+-----------+--------------+-----------------+-------------+-------------+\n",
      "\n",
      "\n",
      "âœ“ Pronto para prÃ³ximas etapas!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# CÃ‰LULA 5: CRIAR VIEW DE NFe (OTIMIZADA + SKIP PERÃODO CORROMPIDO)\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CRIANDO VIEW DE NFe PROCESSADA (OTIMIZADA)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# REFRESH da tabela NFe primeiro\n",
    "print(\"Fazendo REFRESH da tabela nfe.nfe...\")\n",
    "try:\n",
    "    spark.sql(\"REFRESH TABLE nfe.nfe\")\n",
    "    print(\"âœ“ REFRESH executado\")\n",
    "except Exception as e:\n",
    "    print(f\"âš  REFRESH falhou: {e}\")\n",
    "\n",
    "# Materializar os CNPJs em cache\n",
    "spark.sql(\"CACHE TABLE cadastro_cnpj\")\n",
    "cnpj_list = [row.cnpj for row in spark.sql(\"SELECT cnpj FROM cadastro_cnpj\").collect()]\n",
    "print(f\"CNPJs no cadastro: {len(cnpj_list)}\")\n",
    "\n",
    "# Criar string para usar no IN clause\n",
    "cnpj_in_clause = \"', '\".join(cnpj_list)\n",
    "\n",
    "# âš ï¸ AJUSTE CRÃTICO: Pular perÃ­odos corrompidos\n",
    "# Se 202001 estÃ¡ corrompido, comeÃ§ar de 202002\n",
    "PERIODO_INICIO_AJUSTADO = 202101  # ComeÃ§ar de Janeiro/2021 ao invÃ©s de 2020\n",
    "\n",
    "print(f\"\\nâš ï¸ ATENÃ‡ÃƒO: Pulando perÃ­odo 2020 devido a arquivos corrompidos\")\n",
    "print(f\"PerÃ­odo ajustado: {PERIODO_INICIO_AJUSTADO} atÃ© {PERIODO_FIM}\")\n",
    "\n",
    "query_nfe_view = f\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW vw_nfe_processada AS\n",
    "SELECT \n",
    "    -- IdentificaÃ§Ã£o\n",
    "    a.chave AS chave_nfe,\n",
    "    (a.ano_emissao * 100 + a.mes_emissao) AS periodo_ref,\n",
    "    CONCAT(LPAD(a.mes_emissao, 2, '0'), '/', a.ano_emissao) AS periodo_ref_formatado,\n",
    "    a.dhemi_orig AS data_hora_emissao,\n",
    "    \n",
    "    -- Emitente (limpeza feita uma vez)\n",
    "    cnpj_emit AS cnpj_emitente,\n",
    "    a.procnfe.nfe.infnfe.emit.xnome AS razao_social_emitente,\n",
    "    a.procnfe.nfe.infnfe.emit.xfant AS nome_fantasia_emitente,\n",
    "    a.procnfe.nfe.infnfe.emit.ie AS ie_emitente,\n",
    "    a.procnfe.nfe.infnfe.emit.crt AS crt_emitente,\n",
    "    a.procnfe.nfe.infnfe.emit.enderemit.uf AS uf_emitente,\n",
    "    a.procnfe.nfe.infnfe.emit.enderemit.xmun AS municipio_emitente,\n",
    "    \n",
    "    -- DestinatÃ¡rio (limpeza feita uma vez)\n",
    "    cnpj_dest AS cnpj_destinatario,\n",
    "    a.procnfe.nfe.infnfe.dest.xnome AS razao_social_destinatario,\n",
    "    a.procnfe.nfe.infnfe.dest.ie AS ie_destinatario,\n",
    "    a.procnfe.nfe.infnfe.dest.indiedest AS indicador_ie_destinatario,\n",
    "    a.procnfe.nfe.infnfe.dest.enderdest.uf AS uf_destinatario,\n",
    "    a.procnfe.nfe.infnfe.dest.enderdest.xmun AS municipio_destinatario,\n",
    "    \n",
    "    -- OperaÃ§Ã£o\n",
    "    a.procnfe.nfe.infnfe.ide.natop AS natureza_operacao,\n",
    "    a.procnfe.nfe.infnfe.ide.tpnf AS tipo_nf,\n",
    "    CASE \n",
    "        WHEN a.procnfe.nfe.infnfe.ide.tpnf = 0 THEN 'Entrada'\n",
    "        WHEN a.procnfe.nfe.infnfe.ide.tpnf = 1 THEN 'Saida'\n",
    "        ELSE 'Indefinido'\n",
    "    END AS entrada_saida,\n",
    "    CASE \n",
    "        WHEN a.procnfe.nfe.infnfe.ide.indfinal = 1 THEN 'Consumidor final'\n",
    "        ELSE 'Normal'\n",
    "    END AS tipo_consumidor,\n",
    "    \n",
    "    -- Itens\n",
    "    b._nitem AS numero_item,\n",
    "    b.prod.cprod AS codigo_produto,\n",
    "    b.prod.xprod AS descricao_produto,\n",
    "    b.prod.ncm AS ncm,\n",
    "    b.prod.cfop AS cfop,\n",
    "    \n",
    "    -- Valores\n",
    "    CAST(COALESCE(b.prod.vprod, 0) AS DECIMAL(15,2)) AS valor_produto,\n",
    "    CAST(COALESCE(b.prod.vfrete, 0) AS DECIMAL(15,2)) AS valor_frete,\n",
    "    CAST(COALESCE(b.prod.vseg, 0) AS DECIMAL(15,2)) AS valor_seguro,\n",
    "    CAST(COALESCE(b.prod.vdesc, 0) AS DECIMAL(15,2)) AS valor_desconto,\n",
    "    CAST(COALESCE(b.prod.voutro, 0) AS DECIMAL(15,2)) AS valor_outras_despesas,\n",
    "    \n",
    "    -- ICMS\n",
    "    b.imposto.icms.resumo.cst AS cst_icms,\n",
    "    CAST(COALESCE(b.imposto.icms.resumo.vbc, 0) AS DECIMAL(15,2)) AS bc_icms,\n",
    "    CAST(COALESCE(b.imposto.icms.resumo.picms, 0) AS DECIMAL(7,4)) AS aliquota_icms,\n",
    "    CAST(COALESCE(b.imposto.icms.resumo.vicms, 0) AS DECIMAL(15,2)) AS valor_icms,\n",
    "    CAST(COALESCE(b.imposto.icms.resumo.vcredicmssn, 0) AS DECIMAL(15,2)) AS valor_credito_sn,\n",
    "    \n",
    "    -- Totais NFe\n",
    "    CAST(COALESCE(a.procnfe.nfe.infnfe.total.icmstot.vnf, 0) AS DECIMAL(15,2)) AS total_nfe,\n",
    "    \n",
    "    -- Flags cadastro\n",
    "    IF(cnpj_emit IN ('{cnpj_in_clause}'), 1, 0) AS emitente_no_cadastro,\n",
    "    IF(cnpj_dest IN ('{cnpj_in_clause}'), 1, 0) AS destinatario_no_cadastro\n",
    "\n",
    "FROM (\n",
    "    SELECT \n",
    "        *,\n",
    "        REGEXP_REPLACE(TRIM(CAST(procnfe.nfe.infnfe.emit.cnpj AS STRING)), '[^0-9]', '') AS cnpj_emit,\n",
    "        REGEXP_REPLACE(TRIM(CAST(procnfe.nfe.infnfe.dest.cnpj AS STRING)), '[^0-9]', '') AS cnpj_dest\n",
    "    FROM nfe.nfe\n",
    "    WHERE situacao = 1\n",
    "      AND (ano_emissao * 100 + mes_emissao) >= {PERIODO_INICIO_AJUSTADO}\n",
    "      AND (ano_emissao * 100 + mes_emissao) <= {PERIODO_FIM}\n",
    ") a\n",
    "LATERAL VIEW EXPLODE(a.procnfe.nfe.infnfe.det) exploded_table AS b\n",
    "WHERE cnpj_emit IN ('{cnpj_in_clause}')\n",
    "   OR cnpj_dest IN ('{cnpj_in_clause}')\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    spark.sql(query_nfe_view)\n",
    "    print(\"âœ“ View criada\")\n",
    "    \n",
    "    # Verificar SEM executar count completo (muito pesado)\n",
    "    print(\"\\nâœ“ View NFe criada com sucesso\")\n",
    "    print(f\"PerÃ­odo: {PERIODO_INICIO_AJUSTADO} atÃ© {PERIODO_FIM}\")\n",
    "    \n",
    "    # Amostra para verificar\n",
    "    print(\"\\nAmostra dos dados (5 registros):\")\n",
    "    spark.sql(\"SELECT periodo_ref, cnpj_emitente, cnpj_destinatario, entrada_saida, valor_produto FROM vw_nfe_processada LIMIT 5\").show(truncate=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Erro: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"\\nâœ“ Pronto para prÃ³ximas etapas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c6c35c-c809-489f-b57f-2e1eaf9069b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "INTEGRANDO INFORMAÃ‡Ã•ES DE CFOP\n",
      "================================================================================\n",
      "âœ“ View vw_nfe_com_cfop criada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Total: 3,818 registros\n",
      "\n",
      "Top 10 CFOPs:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:==>       (6201 + 28) / 27177][Stage 27:>                 (0 + 0) / 1]"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# CÃ‰LULA 6: INTEGRAR CFOP (Execute apÃ³s a cÃ©lula 5 terminar)\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"INTEGRANDO INFORMAÃ‡Ã•ES DE CFOP\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "query_cfop_view = \"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW vw_nfe_com_cfop AS\n",
    "SELECT \n",
    "    nfe.*,\n",
    "    cfop.conta,\n",
    "    cfop.descricaocfop,\n",
    "    cfop.indcom,\n",
    "    cfop.movimento\n",
    "FROM vw_nfe_processada nfe\n",
    "LEFT JOIN niat.tabela_cfop cfop \n",
    "    ON nfe.cfop = cfop.cfop\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query_cfop_view)\n",
    "print(\"âœ“ View vw_nfe_com_cfop criada\")\n",
    "\n",
    "# Verificar\n",
    "total = spark.sql(\"SELECT COUNT(*) as total FROM vw_nfe_com_cfop\").collect()[0]['total']\n",
    "print(f\"âœ“ Total: {total:,} registros\")\n",
    "\n",
    "# Top CFOPs\n",
    "print(\"\\nTop 10 CFOPs:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        cfop,\n",
    "        descricaocfop,\n",
    "        conta,\n",
    "        entrada_saida,\n",
    "        COUNT(*) as qtde,\n",
    "        ROUND(SUM(valor_produto), 2) as vl_total\n",
    "    FROM vw_nfe_com_cfop\n",
    "    GROUP BY cfop, descricaocfop, conta, entrada_saida\n",
    "    ORDER BY qtde DESC\n",
    "    LIMIT 10\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d783072-7f84-43f0-b8bb-e4901db6372a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# CÃ‰LULA 6: INTEGRAR CFOP (CORRIGIDA E Refatorada - API)\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"INTEGRANDO INFORMAÃ‡Ã•ES DE CFOP\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Primeiro, carregar a tabela CFOP para memÃ³ria\n",
    "try:\n",
    "    print(\"Carregando tabela CFOP...\")\n",
    "    df_cfop = spark.table(\"niat.tabela_cfop\").select(\n",
    "        \"cfop\", \"conta\", \"descricaocfop\", \"indcom\", \n",
    "        \"movimento\", \"especial\", \"local\", \"mercenergtel\"\n",
    "    )\n",
    "    \n",
    "    # Criar view temporÃ¡ria e cachear\n",
    "    df_cfop.createOrReplaceTempView(\"temp_cfop\")\n",
    "    spark.sql(\"CACHE TABLE temp_cfop\")\n",
    "    total_cfops = spark.table(\"temp_cfop\").count()\n",
    "    print(f\"âœ“ Tabela CFOP cacheada: {total_cfops} registros\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Erro ao carregar tabela CFOP: {e}\")\n",
    "    print(\"\\nCriando tabela CFOP bÃ¡sica manualmente...\")\n",
    "    \n",
    "    # Se nÃ£o conseguir acessar, criar uma bÃ¡sica\n",
    "    cfop_data = [\n",
    "        (1101, 0, \"Compra para industrializaÃ§Ã£o\", \"IndustrializaÃ§Ã£o\", \"OperaÃ§Ã£o\"),\n",
    "        (1102, 0, \"Compra para comercializaÃ§Ã£o\", \"ComercializaÃ§Ã£o\", \"OperaÃ§Ã£o\"),\n",
    "        (5101, 1, \"Venda de produÃ§Ã£o do estabelecimento\", \"IndustrializaÃ§Ã£o\", \"OperaÃ§Ã£o\"),\n",
    "        (5102, 1, \"Venda de mercadoria adquirida\", \"ComercializaÃ§Ã£o\", \"OperaÃ§Ã£o\"),\n",
    "        (5403, 1, \"Venda de mercadoria em operaÃ§Ã£o com nÃ£o contribuinte\", \"ComercializaÃ§Ã£o\", \"OperaÃ§Ã£o\"),\n",
    "        (6101, 1, \"Venda de produÃ§Ã£o interestadual\", \"IndustrializaÃ§Ã£o\", \"OperaÃ§Ã£o\"),\n",
    "        (6102, 1, \"Venda de mercadoria interestadual\", \"ComercializaÃ§Ã£o\", \"OperaÃ§Ã£o\"),\n",
    "    ]\n",
    "    \n",
    "    from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "    schema = StructType([\n",
    "        StructField(\"cfop\", IntegerType(), True),\n",
    "        StructField(\"conta\", IntegerType(), True),\n",
    "        StructField(\"descricaocfop\", StringType(), True),\n",
    "        StructField(\"indcom\", StringType(), True),\n",
    "        StructField(\"movimento\", StringType(), True),\n",
    "    ])\n",
    "    \n",
    "    df_cfop = spark.createDataFrame(cfop_data, schema)\n",
    "    df_cfop.createOrReplaceTempView(\"temp_cfop\")\n",
    "    spark.sql(\"CACHE TABLE temp_cfop\")\n",
    "    print(\"âœ“ Tabela CFOP bÃ¡sica criada e cacheada\")\n",
    "\n",
    "# Agora fazer o join com a view temporÃ¡ria usando API\n",
    "df_nfe_processada = spark.table(\"vw_nfe_processada\")\n",
    "df_cfop_cached = spark.table(\"temp_cfop\")\n",
    "\n",
    "df_nfe_com_cfop = df_nfe_processada.join(\n",
    "    df_cfop_cached,\n",
    "    df_nfe_processada.cfop == df_cfop_cached.cfop,\n",
    "    \"left\"\n",
    ").select(\n",
    "    df_nfe_processada[\"*\"],\n",
    "    df_cfop_cached[\"conta\"],\n",
    "    df_cfop_cached[\"descricaocfop\"],\n",
    "    df_cfop_cached[\"indcom\"],\n",
    "    df_cfop_cached[\"movimento\"]\n",
    ")\n",
    "\n",
    "df_nfe_com_cfop.createOrReplaceTempView(\"vw_nfe_com_cfop\")\n",
    "print(\"âœ“ View vw_nfe_com_cfop criada\")\n",
    "\n",
    "# âš  REMOVIDO: O COUNT(*) aqui era lento e desnecessÃ¡rio.\n",
    "# O Spark executarÃ¡ a view quando for realmente usada.\n",
    "\n",
    "# Top CFOPs (para verificaÃ§Ã£o)\n",
    "print(\"\\nTop 10 CFOPs (amostra):\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        cfop,\n",
    "        descricaocfop,\n",
    "        conta,\n",
    "        entrada_saida,\n",
    "        COUNT(*) as qtde,\n",
    "        ROUND(SUM(valor_produto), 2) as vl_total\n",
    "    FROM vw_nfe_com_cfop\n",
    "    GROUP BY cfop, descricaocfop, conta, entrada_saida\n",
    "    ORDER BY qtde DESC\n",
    "    LIMIT 10\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0853f976-703d-4b6c-9613-a17955245790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# CÃ‰LULA 7: CALCULAR ICMS\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CALCULANDO ICMS - DÃ‰BITOS E CRÃ‰DITOS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "query_icms = \"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW vw_nfe_com_icms AS\n",
    "SELECT \n",
    "    *,\n",
    "    -- BC total do item\n",
    "    (valor_produto + valor_frete + valor_seguro + valor_outras_despesas - valor_desconto) AS bc_total_item,\n",
    "    \n",
    "    -- DÃ‰BITO (SaÃ­da)\n",
    "    CASE \n",
    "        WHEN entrada_saida = 'Saida' AND conta = 1 THEN\n",
    "            CASE\n",
    "                WHEN valor_icms > 0 THEN valor_icms\n",
    "                WHEN bc_icms > 0 AND aliquota_icms > 0 THEN bc_icms * (aliquota_icms / 100)\n",
    "                WHEN tipo_consumidor = 'Consumidor final' THEN \n",
    "                    (valor_produto + valor_frete + valor_seguro + valor_outras_despesas - valor_desconto) * 0.17\n",
    "                WHEN uf_destinatario IN ('MG', 'PR', 'RJ', 'RS', 'SP') AND uf_destinatario != uf_emitente THEN\n",
    "                    (valor_produto + valor_frete + valor_seguro + valor_outras_despesas - valor_desconto) * 0.12\n",
    "                WHEN uf_destinatario != uf_emitente THEN\n",
    "                    (valor_produto + valor_frete + valor_seguro + valor_outras_despesas - valor_desconto) * 0.07\n",
    "                ELSE \n",
    "                    (valor_produto + valor_frete + valor_seguro + valor_outras_despesas - valor_desconto) * 0.12\n",
    "            END\n",
    "        ELSE 0\n",
    "    END AS vl_icms_debito,\n",
    "    \n",
    "    CASE \n",
    "        WHEN entrada_saida = 'Saida' AND conta = 1 THEN\n",
    "            COALESCE(bc_icms, valor_produto + valor_frete + valor_seguro + valor_outras_despesas - valor_desconto)\n",
    "        ELSE 0\n",
    "    END AS bc_icms_debito,\n",
    "    \n",
    "    -- CRÃ‰DITO (Entrada)\n",
    "    CASE \n",
    "        WHEN entrada_saida = 'Entrada' AND conta = -1 \n",
    "             AND cst_icms NOT IN ('10', '15', '30', '40', '41', '53', '60', '61', '70', '102', '202', '203', '300', '500') THEN\n",
    "            CASE\n",
    "                WHEN valor_icms > 0 THEN valor_icms\n",
    "                WHEN valor_credito_sn > 0 THEN valor_credito_sn\n",
    "                ELSE 0\n",
    "            END\n",
    "        ELSE 0\n",
    "    END AS vl_icms_credito,\n",
    "    \n",
    "    CASE \n",
    "        WHEN entrada_saida = 'Entrada' AND conta = -1 THEN\n",
    "            COALESCE(bc_icms, valor_produto + valor_frete + valor_seguro + valor_outras_despesas - valor_desconto)\n",
    "        ELSE 0\n",
    "    END AS bc_icms_credito\n",
    "\n",
    "FROM vw_nfe_com_cfop\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query_icms)\n",
    "print(\"âœ“ View vw_nfe_com_icms criada\")\n",
    "\n",
    "# Resumo\n",
    "print(\"\\nðŸ“Š RESUMO ICMS:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        entrada_saida,\n",
    "        COUNT(*) as qtde_itens,\n",
    "        ROUND(SUM(bc_icms_debito), 2) as bc_debito,\n",
    "        ROUND(SUM(vl_icms_debito), 2) as vl_debito,\n",
    "        ROUND(SUM(bc_icms_credito), 2) as bc_credito,\n",
    "        ROUND(SUM(vl_icms_credito), 2) as vl_credito\n",
    "    FROM vw_nfe_com_icms\n",
    "    GROUP BY entrada_saida\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0166b7-58ce-4a8b-a5e7-9bac3e2bbd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# CÃ‰LULA 8: ANÃLISE POR ENTRADA/SAÃDA E EMITENTE/DESTINATÃRIO\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SEPARANDO: ENTRADA vs SAÃDA | EMITENTE vs DESTINATÃRIO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ENTRADA - DESTINATÃRIO (empresa do cadastro recebeu mercadoria)\n",
    "df_entrada_dest = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM vw_nfe_com_icms\n",
    "    WHERE entrada_saida = 'Entrada'\n",
    "      AND destinatario_no_cadastro = 1\n",
    "\"\"\")\n",
    "df_entrada_dest.createOrReplaceTempView(\"vw_entrada_destinatario\")\n",
    "# âš  REMOVIDO: count() desnecessÃ¡rio\n",
    "print(f\"âœ“ View 'vw_entrada_destinatario' criada.\")\n",
    "\n",
    "\n",
    "# ENTRADA - EMITENTE (empresa do cadastro emitiu nota de devoluÃ§Ã£o/retorno)\n",
    "df_entrada_emit = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM vw_nfe_com_icms\n",
    "    WHERE entrada_saida = 'Entrada'\n",
    "      AND emitente_no_cadastro = 1\n",
    "\"\"\")\n",
    "df_entrada_emit.createOrReplaceTempView(\"vw_entrada_emitente\")\n",
    "# âš  REMOVIDO: count() desnecessÃ¡rio\n",
    "print(f\"âœ“ View 'vw_entrada_emitente' criada.\")\n",
    "\n",
    "# SAÃDA - EMITENTE (empresa do cadastro vendeu)\n",
    "df_saida_emit = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM vw_nfe_com_icms\n",
    "    WHERE entrada_saida = 'Saida'\n",
    "      AND emitente_no_cadastro = 1\n",
    "\"\"\")\n",
    "df_saida_emit.createOrReplaceTempView(\"vw_saida_emitente\")\n",
    "# âš  REMOVIDO: count() desnecessÃ¡rio\n",
    "print(f\"âœ“ View 'vw_saida_emitente' criada.\")\n",
    "\n",
    "# SAÃDA - DESTINATÃRIO (empresa do cadastro recebeu venda de terceiro)\n",
    "df_saida_dest = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM vw_nfe_com_icms\n",
    "    WHERE entrada_saida = 'Saida'\n",
    "      AND destinatario_no_cadastro = 1\n",
    "\"\"\")\n",
    "df_saida_dest.createOrReplaceTempView(\"vw_saida_destinatario\")\n",
    "# âš  REMOVIDO: count() desnecessÃ¡rio\n",
    "print(f\"âœ“ View 'vw_saida_destinatario' criada.\")\n",
    "\n",
    "print(f\"\\nâœ“ Views de separaÃ§Ã£o prontas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab4ae42-feb7-441d-a528-5ac17b13c738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# CÃ‰LULA 9: PROCESSAR CTe (CORRIGIDA - ESTRUTURA ICMS)\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PROCESSANDO CTe - CRÃ‰DITO DE FRETE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Obter a lista de CNPJs (jÃ¡ deve estar em cache de 'cadastro_cnpj')\n",
    "cnpj_list_cte = [row.cnpj for row in spark.sql(\"SELECT cnpj FROM cadastro_cnpj\").collect()]\n",
    "cnpj_in_clause = \"','\".join(cnpj_list_cte) # Para o SQL\n",
    "print(f\"Processando CTe para {len(cnpj_list_cte)} CNPJs.\")\n",
    "\n",
    "query_cte = f\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW vw_cte_processada AS\n",
    "SELECT \n",
    "    -- IdentificaÃ§Ã£o\n",
    "    a.chave AS chave_cte,\n",
    "    a.ano_emissao AS ano_emissao,\n",
    "    \n",
    "    -- PerÃ­odo\n",
    "    CONCAT(\n",
    "        LPAD(MONTH(a.proccte.cte.infcte.ide.dhemi), 2, '0'), \n",
    "        '/', \n",
    "        YEAR(a.proccte.cte.infcte.ide.dhemi)\n",
    "    ) AS periodo_ref,\n",
    "    \n",
    "    (YEAR(a.proccte.cte.infcte.ide.dhemi) * 100 + MONTH(a.proccte.cte.infcte.ide.dhemi)) AS periodo_ref_num,\n",
    "    \n",
    "    a.proccte.cte.infcte.ide.dhemi AS data_hora_emissao,\n",
    "    \n",
    "    -- Tomador (quem paga o frete)\n",
    "    REGEXP_REPLACE(\n",
    "        TRIM(CAST(a.proccte.cte.infcte.ide.toma4.cnpj AS STRING)), \n",
    "        '[^0-9]', ''\n",
    "    ) AS cnpj_tomador,\n",
    "    \n",
    "    a.proccte.cte.infcte.ide.toma4.xnome AS nome_tomador,\n",
    "    a.proccte.cte.infcte.ide.toma4.toma AS indicador_tomador,\n",
    "    \n",
    "    -- Remetente\n",
    "    REGEXP_REPLACE(\n",
    "        TRIM(CAST(a.proccte.cte.infcte.rem.cnpj AS STRING)), \n",
    "        '[^0-9]', ''\n",
    "    ) AS cnpj_remetente,\n",
    "    \n",
    "    a.proccte.cte.infcte.rem.xnome AS nome_remetente,\n",
    "    \n",
    "    -- DestinatÃ¡rio\n",
    "    REGEXP_REPLACE(\n",
    "        TRIM(CAST(a.proccte.cte.infcte.dest.cnpj AS STRING)), \n",
    "        '[^0-9]', ''\n",
    "    ) AS cnpj_destinatario,\n",
    "    \n",
    "    a.proccte.cte.infcte.dest.xnome AS nome_destinatario,\n",
    "    \n",
    "    -- Emitente (transportadora)\n",
    "    REGEXP_REPLACE(\n",
    "        TRIM(CAST(a.proccte.cte.infcte.emit.cnpj AS STRING)), \n",
    "        '[^0-9]', ''\n",
    "    ) AS cnpj_emitente,\n",
    "    \n",
    "    a.proccte.cte.infcte.emit.xnome AS nome_emitente,\n",
    "    a.proccte.cte.infcte.emit.ie AS ie_emitente,\n",
    "    \n",
    "    -- CFOP e Natureza\n",
    "    a.proccte.cte.infcte.ide.cfop AS cfop,\n",
    "    a.proccte.cte.infcte.ide.natop AS natureza_operacao,\n",
    "    a.proccte.cte.infcte.ide.modal AS modal,\n",
    "    a.proccte.cte.infcte.ide.tpserv AS tipo_servico,\n",
    "    \n",
    "    -- Valores\n",
    "    CAST(COALESCE(a.proccte.cte.infcte.vprest.vtprest, 0) AS DECIMAL(15,2)) AS valor_total_servico,\n",
    "    \n",
    "    -- ICMS - Estrutura CORRETA baseada no describe\n",
    "    CAST(COALESCE(\n",
    "        a.proccte.cte.infcte.imp.icms.icms00.vicms,\n",
    "        a.proccte.cte.infcte.imp.icms.icms20.vicms,\n",
    "        a.proccte.cte.infcte.imp.icms.icms60.vcred,\n",
    "        a.proccte.cte.infcte.imp.icms.icms90.vcred,\n",
    "        a.proccte.cte.infcte.imp.icms.cst00.vicms,\n",
    "        a.proccte.cte.infcte.imp.icms.cst20.vicms,\n",
    "        a.proccte.cte.infcte.imp.icms.cst80.vcred,\n",
    "        a.proccte.cte.infcte.imp.icms.cst90.vcred,\n",
    "        0\n",
    "    ) AS DECIMAL(15,2)) AS valor_icms,\n",
    "    \n",
    "    CAST(COALESCE(\n",
    "        a.proccte.cte.infcte.imp.icms.icms00.vbc,\n",
    "        a.proccte.cte.infcte.imp.icms.icms20.vbc,\n",
    "        a.proccte.cte.infcte.imp.icms.icms90.vbc,\n",
    "        a.proccte.cte.infcte.imp.icms.cst00.vbc,\n",
    "        a.proccte.cte.infcte.imp.icms.cst20.vbc,\n",
    "        a.proccte.cte.infcte.imp.icms.cst80.vbc,\n",
    "        a.proccte.cte.infcte.imp.icms.cst90.vbc,\n",
    "        0\n",
    "    ) AS DECIMAL(15,2)) AS bc_icms,\n",
    "    \n",
    "    -- CST\n",
    "    COALESCE(\n",
    "        a.proccte.cte.infcte.imp.icms.icms00.cst,\n",
    "        a.proccte.cte.infcte.imp.icms.icms20.cst,\n",
    "        a.proccte.cte.infcte.imp.icms.icms45.cst,\n",
    "        a.proccte.cte.infcte.imp.icms.icms60.cst,\n",
    "        a.proccte.cte.infcte.imp.icms.icms90.cst,\n",
    "        a.proccte.cte.infcte.imp.icms.icmssn.cst,\n",
    "        a.proccte.cte.infcte.imp.icms.cst00.cst,\n",
    "        a.proccte.cte.infcte.imp.icms.cst20.cst,\n",
    "        a.proccte.cte.infcte.imp.icms.cst45.cst,\n",
    "        a.proccte.cte.infcte.imp.icms.cst80.cst,\n",
    "        a.proccte.cte.infcte.imp.icms.cst90.cst\n",
    "    ) AS cst_icms\n",
    "\n",
    "FROM cte.cte a\n",
    "WHERE a.situacao = 1\n",
    "  AND a.ano_emissao >= {PERIODO_INICIO // 100}\n",
    "  AND a.ano_emissao <= {PERIODO_FIM // 100}\n",
    "  AND a.proccte.cte.infcte.ide.toma4.cnpj IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    spark.sql(query_cte)\n",
    "    print(\"âœ“ View vw_cte_processada criada\")\n",
    "    \n",
    "    # Filtrar CTe relevantes\n",
    "    query_cte_filtrado = f\"\"\"\n",
    "    CREATE OR REPLACE TEMPORARY VIEW vw_cte_filtrado AS\n",
    "    SELECT \n",
    "        *,\n",
    "        IF(cnpj_tomador IN ('{cnpj_in_clause}'), 1, 0) AS tomador_no_cadastro,\n",
    "        IF(cnpj_destinatario IN ('{cnpj_in_clause}'), 1, 0) AS destinatario_no_cadastro,\n",
    "        IF(cnpj_remetente IN ('{cnpj_in_clause}'), 1, 0) AS remetente_no_cadastro\n",
    "    FROM vw_cte_processada\n",
    "    WHERE periodo_ref_num >= {PERIODO_INICIO}\n",
    "      AND periodo_ref_num <= {PERIODO_FIM}\n",
    "      AND (\n",
    "          cnpj_tomador IN ('{cnpj_in_clause}')\n",
    "          OR cnpj_destinatario IN ('{cnpj_in_clause}')\n",
    "          OR cnpj_remetente IN ('{cnpj_in_clause}')\n",
    "      )\n",
    "    \"\"\"\n",
    "    \n",
    "    spark.sql(query_cte_filtrado)\n",
    "    print(f\"âœ“ View 'vw_cte_filtrado' criada.\")\n",
    "    \n",
    "    # âš  REMOVIDO: count() desnecessÃ¡rio.\n",
    "    # Vamos verificar com um LIMIT 5 se hÃ¡ dados.\n",
    "    \n",
    "    print(\"\\nVerificando amostra de CTe (LIMIT 5):\")\n",
    "    amostra_cte = spark.sql(\"SELECT * FROM vw_cte_filtrado LIMIT 5\").collect()\n",
    "    \n",
    "    if len(amostra_cte) > 0:\n",
    "        print(f\"âœ“ CTe encontrados. Exibindo {len(amostra_cte)} registro(s) de amostra:\")\n",
    "        spark.sql(\"SELECT * FROM vw_cte_filtrado LIMIT 5\").show(truncate=False)\n",
    "\n",
    "        # Resumo por perÃ­odo\n",
    "        print(\"\\nResumo CTe por perÃ­odo:\")\n",
    "        spark.sql(\"\"\"\n",
    "            SELECT \n",
    "                periodo_ref,\n",
    "                COUNT(*) as qtde_cte,\n",
    "                ROUND(SUM(valor_total_servico), 2) as vl_total_servico,\n",
    "                ROUND(SUM(valor_icms), 2) as vl_icms_credito\n",
    "            FROM vw_cte_filtrado\n",
    "            GROUP BY periodo_ref\n",
    "            ORDER BY periodo_ref\n",
    "        \"\"\").show(20, truncate=False)\n",
    "        \n",
    "    else:\n",
    "        print(\"\\nâš  Nenhum CTe encontrado para os CNPJs do cadastro no perÃ­odo\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Erro ao processar CTe: {e}\")\n",
    "    print(\"\\nâš  Criando view CTe vazia para continuar processamento...\")\n",
    "    \n",
    "    # Criar view vazia\n",
    "    spark.sql(\"\"\"\n",
    "        CREATE OR REPLACE TEMPORARY VIEW vw_cte_filtrado AS\n",
    "        SELECT \n",
    "            CAST(NULL AS STRING) AS periodo_ref,\n",
    "            CAST(NULL AS INT) AS periodo_ref_num,\n",
    "            CAST(NULL AS STRING) AS cnpj_tomador,\n",
    "            CAST(0 AS DECIMAL(15,2)) AS valor_icms,\n",
    "            CAST(0 AS INT) AS tomador_no_cadastro\n",
    "        WHERE 1=0\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"âœ“ View CTe vazia criada - processamento continuarÃ¡ sem dados de CTe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2350d0c-439f-4e40-8a18-1ca6dc4d8102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# CÃ‰LULA 10: CARREGAR PGDAS-D (Refatorado - API)\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CARREGANDO DADOS PGDAS-D (SIMPLES NACIONAL)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Lista de CNPJs jÃ¡ deve estar em cache de 'cadastro_cnpj'\n",
    "cnpj_list_pgdas = [row.cnpj for row in spark.sql(\"SELECT cnpj FROM cadastro_cnpj\").collect()]\n",
    "print(f\"Processando PGDAS-D para {len(cnpj_list_pgdas)} CNPJs.\")\n",
    "\n",
    "try:\n",
    "    df_pgdas_raw = spark.table(\"usr_sat_ods.sna_pgdasd_estabelecimento_raw\")\n",
    "\n",
    "    df_pgdas_filtrado = (df_pgdas_raw\n",
    "        .withColumn(\"cnpj\", regexp_replace(trim(col(\"nu_cnpj\")), '[^0-9]', ''))\n",
    "        .filter(\n",
    "            (col(\"nu_per_ref\") >= PERIODO_INICIO) &\n",
    "            (col(\"nu_per_ref\") <= PERIODO_FIM) &\n",
    "            (col(\"cnpj\").isin(cnpj_list_pgdas))\n",
    "        )\n",
    "        .select(\n",
    "            col(\"cnpj\"),\n",
    "            col(\"nu_per_ref\").alias(\"periodo_ref\"),\n",
    "            concat(\n",
    "                lpad((col(\"nu_per_ref\") % 100).cast(\"string\"), 2, '0'),\n",
    "                lit('/'),\n",
    "                floor(col(\"nu_per_ref\") / 100).cast(\"string\")\n",
    "            ).alias(\"periodo_formatado\"),\n",
    "            \n",
    "            coalesce(col(\"vl_rec_bruta_estab\"), lit(0)).cast(\"decimal(15,2)\").alias(\"receita_bruta\"),\n",
    "            coalesce(col(\"vl_icms_sc\"), lit(0)).cast(\"decimal(15,2)\").alias(\"icms_declarado_sc\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    df_pgdas_filtrado.createOrReplaceTempView(\"vw_pgdas_filtrado\")\n",
    "    \n",
    "    # âš  REMOVIDO: count() desnecessÃ¡rio.\n",
    "    print(f\"âœ“ View 'vw_pgdas_filtrado' criada.\")\n",
    "\n",
    "    # Mostrar amostra\n",
    "    print(\"\\nAmostra PGDAS-D:\")\n",
    "    spark.sql(\"\"\"\n",
    "        SELECT *\n",
    "        FROM vw_pgdas_filtrado\n",
    "        ORDER BY periodo_ref DESC\n",
    "        LIMIT 10\n",
    "    \"\"\").show(10, truncate=False)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Erro ao carregar PGDAS-D: {e}\")\n",
    "    print(\"\\nâš  Criando view PGDAS-D vazia para continuar processamento...\")\n",
    "    \n",
    "    # Criar view vazia\n",
    "    spark.sql(\"\"\"\n",
    "        CREATE OR REPLACE TEMPORARY VIEW vw_pgdas_filtrado AS\n",
    "        SELECT \n",
    "            CAST(NULL AS STRING) AS cnpj,\n",
    "            CAST(NULL AS INT) AS periodo_ref,\n",
    "            CAST(NULL AS STRING) AS periodo_formatado,\n",
    "            CAST(0 AS DECIMAL(15,2)) AS receita_bruta,\n",
    "            CAST(0 AS DECIMAL(15,2)) AS icms_declarado_sc\n",
    "        WHERE 1=0\n",
    "    \"\"\")\n",
    "    print(\"âœ“ View PGDAS-D vazia criada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbffbdc5-d393-473c-a52c-106fa9da9fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# CÃ‰LULA 11: CONSOLIDAR NOTIFICAÃ‡ÃƒO (âœ… PADRÃƒO OURO)\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GERANDO NOTIFICAÃ‡ÃƒO - APURAÃ‡ÃƒO MENSAL DE ICMS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "query_notificacao = \"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW vw_notificacao AS\n",
    "WITH apuracao_saida AS (\n",
    "    SELECT \n",
    "        periodo_ref,\n",
    "        cnpj_emitente AS cnpj,\n",
    "        SUM(bc_icms_debito) AS bc_icms_saida,\n",
    "        SUM(vl_icms_debito) AS vl_icms_saida\n",
    "    FROM vw_saida_emitente\n",
    "    GROUP BY periodo_ref, cnpj_emitente\n",
    "),\n",
    "apuracao_entrada AS (\n",
    "    SELECT \n",
    "        periodo_ref,\n",
    "        cnpj_destinatario AS cnpj,\n",
    "        SUM(bc_icms_credito) AS bc_icms_entrada,\n",
    "        SUM(vl_icms_credito) AS vl_icms_entrada\n",
    "    FROM vw_entrada_destinatario\n",
    "    GROUP BY periodo_ref, cnpj_destinatario\n",
    "),\n",
    "apuracao_cte AS (\n",
    "    SELECT \n",
    "        periodo_ref_num AS periodo_ref,\n",
    "        cnpj_tomador AS cnpj,\n",
    "        SUM(valor_icms) AS vl_icms_cte\n",
    "    FROM vw_cte_filtrado\n",
    "    WHERE tomador_no_cadastro = 1\n",
    "    GROUP BY periodo_ref_num, cnpj_tomador\n",
    ")\n",
    "SELECT \n",
    "    COALESCE(s.periodo_ref, e.periodo_ref, c.periodo_ref, p.periodo_ref) AS periodo,\n",
    "    COALESCE(s.cnpj, e.cnpj, c.cnpj, p.cnpj) AS cnpj,\n",
    "    CONCAT(\n",
    "        LPAD(CAST(COALESCE(s.periodo_ref, e.periodo_ref, c.periodo_ref, p.periodo_ref) % 100 AS STRING), 2, '0'),\n",
    "        '/',\n",
    "        CAST(COALESCE(s.periodo_ref, e.periodo_ref, c.periodo_ref, p.periodo_ref) / 100 AS STRING)\n",
    "    ) AS periodo_formatado,\n",
    "    \n",
    "    -- DÃ©bitos\n",
    "    ROUND(COALESCE(s.bc_icms_saida, 0), 2) AS bc_icms_saida,\n",
    "    ROUND(COALESCE(s.vl_icms_saida, 0), 2) AS vl_icms_saida,\n",
    "    \n",
    "    -- CrÃ©ditos\n",
    "    ROUND(COALESCE(e.bc_icms_entrada, 0), 2) AS bc_icms_entrada,\n",
    "    ROUND(COALESCE(e.vl_icms_entrada, 0), 2) AS vl_icms_entrada,\n",
    "    ROUND(COALESCE(c.vl_icms_cte, 0), 2) AS vl_icms_cte,\n",
    "    \n",
    "    -- PGDAS-D\n",
    "    ROUND(COALESCE(p.receita_bruta, 0), 2) AS receita_bruta_declarada,\n",
    "    ROUND(COALESCE(p.icms_declarado_sc, 0), 2) AS icms_declarado,\n",
    "    \n",
    "    -- CÃ¡lculos\n",
    "    ROUND(COALESCE(s.bc_icms_saida, 0) - COALESCE(p.receita_bruta, 0), 2) AS receita_nao_declarada,\n",
    "    ROUND(CASE \n",
    "              WHEN (COALESCE(s.bc_icms_saida, 0) - COALESCE(p.receita_bruta, 0)) > 0 \n",
    "              THEN (COALESCE(s.bc_icms_saida, 0) - COALESCE(p.receita_bruta, 0)) * 0.17 \n",
    "              ELSE 0 \n",
    "          END, 2) AS vl_icms_receita_omitida,\n",
    "    \n",
    "    -- ICMS DEVIDO\n",
    "    ROUND(\n",
    "        COALESCE(s.vl_icms_saida, 0) +\n",
    "        (CASE \n",
    "            WHEN (COALESCE(s.bc_icms_saida, 0) - COALESCE(p.receita_bruta, 0)) > 0 \n",
    "            THEN (COALESCE(s.bc_icms_saida, 0) - COALESCE(p.receita_bruta, 0)) * 0.17 \n",
    "            ELSE 0 \n",
    "         END) -\n",
    "        COALESCE(e.vl_icms_entrada, 0) -\n",
    "        COALESCE(c.vl_icms_cte, 0) -\n",
    "        COALESCE(p.icms_declarado_sc, 0),\n",
    "        2\n",
    "    ) AS vl_icms_devido,\n",
    "    \n",
    "    -- Vencimento (dia 10 do mÃªs seguinte)\n",
    "    DATE_FORMAT(\n",
    "        ADD_MONTHS(\n",
    "            CONCAT(\n",
    "                CAST(COALESCE(s.periodo_ref, e.periodo_ref, c.periodo_ref, p.periodo_ref) / 100 AS INT), '-',\n",
    "                LPAD(CAST(COALESCE(s.periodo_ref, e.periodo_ref, c.periodo_ref, p.periodo_ref) % 100 AS INT), 2, '0'), '-01'\n",
    "            ),\n",
    "            1\n",
    "        ),\n",
    "        'yyyy-MM-10'\n",
    "    ) AS data_vencimento\n",
    "\n",
    "FROM apuracao_saida s\n",
    "FULL OUTER JOIN apuracao_entrada e \n",
    "    ON s.periodo_ref = e.periodo_ref AND s.cnpj = e.cnpj\n",
    "FULL OUTER JOIN apuracao_cte c \n",
    "    ON COALESCE(s.periodo_ref, e.periodo_ref) = c.periodo_ref \n",
    "    AND COALESCE(s.cnpj, e.cnpj) = c.cnpj\n",
    "FULL OUTER JOIN vw_pgdas_filtrado p \n",
    "    ON COALESCE(s.periodo_ref, e.periodo_ref, c.periodo_ref) = p.periodo_ref\n",
    "    AND COALESCE(s.cnpj, e.cnpj, c.cnpj) = p.cnpj\n",
    "ORDER BY cnpj, periodo\n",
    "\"\"\"\n",
    "\n",
    "# ===================================================================\n",
    "# âœ… APLICAÃ‡ÃƒO DO PADRÃƒO OURO (Contar -> Decidir -> Limitar -> toPandas)\n",
    "# ===================================================================\n",
    "try:\n",
    "    spark.sql(query_notificacao)\n",
    "    print(\"âœ“ View vw_notificacao criada\")\n",
    "    \n",
    "    # Materializar resultado para evitar reprocessamento\n",
    "    print(\"\\nMaterializando dados da notificaÃ§Ã£o...\")\n",
    "    df_notificacao = spark.sql(\"SELECT * FROM vw_notificacao ORDER BY cnpj, periodo\")\n",
    "    df_notificacao.cache()  # Cachear para contar e agregar\n",
    "    \n",
    "    # 1. CONTAR\n",
    "    total_periodos = df_notificacao.count()\n",
    "    print(f\"âœ“ Total de perÃ­odos apurados: {total_periodos}\")\n",
    "    \n",
    "    LIMITE_PANDAS_GRAFICOS = 100000 # Limite seguro para grÃ¡ficos\n",
    "    df_notif_pandas = pd.DataFrame() # Inicializa vazio\n",
    "    \n",
    "    # 2. DECIDIR\n",
    "    if total_periodos == 0:\n",
    "        print(\"\\nâš  Nenhum perÃ­odo com dados para apuraÃ§Ã£o\")\n",
    "        df_notif_pandas = pd.DataFrame()\n",
    "        \n",
    "    elif total_periodos > LIMITE_PANDAS_GRAFICOS:\n",
    "        print(f\"\\nâš  Muitos registros ({total_periodos:,}). Limitando a {LIMITE_PANDAS_GRAFICOS} para visualizaÃ§Ã£o...\")\n",
    "        # 3. LIMITAR E COLETAR\n",
    "        df_notif_pandas = df_notificacao.limit(LIMITE_PANDAS_GRAFICOS).toPandas()\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\nâœ“ Coletando {total_periodos:,} registros para visualizaÃ§Ã£o...\")\n",
    "        # 3. COLETAR\n",
    "        df_notif_pandas = df_notificacao.toPandas()\n",
    "        \n",
    "    # ===================================================================\n",
    "    # âœ… CÃLCULO DE ESTATÃSTICAS SEGURO (Feito no Spark)\n",
    "    # ===================================================================\n",
    "    if total_periodos > 0:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"ESTATÃSTICAS CONSOLIDADAS (Calculado via Spark)\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        # Calcular agregados no Spark (muito mais seguro)\n",
    "        resumo_total_spark = df_notificacao.agg(\n",
    "            spark_sum(\"vl_icms_devido\").alias(\"total_devido\"),\n",
    "            spark_sum(\"vl_icms_saida\").alias(\"total_debitos\"),\n",
    "            spark_sum(\"vl_icms_entrada\").alias(\"total_cred_entrada\"),\n",
    "            spark_sum(\"vl_icms_cte\").alias(\"total_cred_cte\"),\n",
    "            spark_sum(\"icms_declarado\").alias(\"total_declarado\"),\n",
    "            spark_sum(\"receita_nao_declarada\").alias(\"total_rec_omitida\"),\n",
    "            spark_sum(\"vl_icms_receita_omitida\").alias(\"total_icms_rec_omitida\")\n",
    "        ).collect()[0]\n",
    "\n",
    "        print(f\"Total ICMS Devido: R$ {resumo_total_spark['total_devido']:,.2f}\")\n",
    "        print(f\"Total DÃ©bitos (SaÃ­da): R$ {resumo_total_spark['total_debitos']:,.2f}\")\n",
    "        print(f\"Total CrÃ©ditos (Entrada): R$ {resumo_total_spark['total_cred_entrada']:,.2f}\")\n",
    "        print(f\"Total CrÃ©ditos (CTe): R$ {resumo_total_spark['total_cred_cte']:,.2f}\")\n",
    "        print(f\"Total ICMS Declarado: R$ {resumo_total_spark['total_declarado']:,.2f}\")\n",
    "        print(f\"Receita NÃ£o Declarada: R$ {resumo_total_spark['total_rec_omitida']:,.2f}\")\n",
    "        print(f\"ICMS sobre Receita Omitida: R$ {resumo_total_spark['total_icms_rec_omitida']:,.2f}\")\n",
    "\n",
    "        # Resumo por CNPJ (Spark)\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"RESUMO POR CNPJ (Calculado via Spark)\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        resumo_cnpj_spark = df_notificacao.groupBy(\"cnpj\").agg(\n",
    "            spark_count(\"periodo\").alias(\"Qtde_PerÃ­odos\"),\n",
    "            spark_sum(\"vl_icms_saida\").alias(\"Total_DÃ©bitos\"),\n",
    "            spark_sum(\"vl_icms_entrada\").alias(\"Total_CrÃ©d_Entrada\"),\n",
    "            spark_sum(\"vl_icms_cte\").alias(\"Total_CrÃ©d_CTe\"),\n",
    "            spark_sum(\"icms_declarado\").alias(\"Total_Declarado\"),\n",
    "            spark_sum(\"vl_icms_devido\").alias(\"Total_Devido\"),\n",
    "            spark_sum(\"receita_nao_declarada\").alias(\"Rec_Omitida\")\n",
    "        ).orderBy(desc(\"Total_Devido\"))\n",
    "        \n",
    "        resumo_cnpj_spark.show(truncate=False)\n",
    "        \n",
    "    else:\n",
    "        # Se nÃ£o houver dados, df_notif_pandas jÃ¡ Ã© um DataFrame vazio\n",
    "        pass\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\nâœ— Erro ao gerar notificaÃ§Ã£o: {e}\")\n",
    "    df_notif_pandas = pd.DataFrame() # Garante que seja um DF vazio em caso de erro\n",
    "\n",
    "# Limpar o cache (se nÃ£o for usar mais)\n",
    "# df_notificacao.unpersist()\n",
    "# Vamos manter cacheado para a exportaÃ§Ã£o na CÃ©lula 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fabfcf-77ae-487e-b3ed-400a324888e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# CÃ‰LULA 12: VISUALIZAÃ‡Ã•ES E ANÃLISES\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VISUALIZAÃ‡Ã•ES E ANÃLISES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# âœ… VerificaÃ§Ã£o PadrÃ£o Ouro: SÃ³ plote se o DataFrame nÃ£o estiver vazio\n",
    "if not df_notif_pandas.empty:\n",
    "    print(f\"Gerando grÃ¡ficos com base em {len(df_notif_pandas)} registros (limitados).\")\n",
    "    \n",
    "    # Preparar dados\n",
    "    df_plot = df_notif_pandas.copy()\n",
    "    df_plot['periodo'] = df_plot['periodo'].astype(str)\n",
    "\n",
    "    # Criar figura com mÃºltiplos grÃ¡ficos\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "    fig.suptitle('ANÃLISE FISCAL - ICMS (Amostra Limitada)', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # GrÃ¡fico 1: EvoluÃ§Ã£o do ICMS Devido\n",
    "    ax1 = axes[0, 0]\n",
    "    for cnpj in df_plot['cnpj'].unique():\n",
    "        data = df_plot[df_plot['cnpj'] == cnpj]\n",
    "        ax1.plot(data['periodo'], data['vl_icms_devido'], marker='o', label=f'CNPJ {cnpj}')\n",
    "    ax1.set_title('EvoluÃ§Ã£o do ICMS Devido', fontweight='bold')\n",
    "    ax1.set_xlabel('PerÃ­odo')\n",
    "    ax1.set_ylabel('Valor (R$)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "    # GrÃ¡fico 2: DÃ©bitos vs CrÃ©ditos\n",
    "    ax2 = axes[0, 1]\n",
    "    # Usar os dados jÃ¡ agregados pela cÃ©lula anterior seria mais rÃ¡pido\n",
    "    # Mas para fins de plotagem temporal, vamos re-agregar o df_plot\n",
    "    periodos_plot = df_plot['periodo'].unique()\n",
    "    if len(periodos_plot) > 24:\n",
    "        periodos_plot = periodos_plot[-24:] # Limitar aos Ãºltimos 24 perÃ­odos\n",
    "        \n",
    "    data_resumo = df_plot[df_plot['periodo'].isin(periodos_plot)].groupby('periodo').agg({\n",
    "        'vl_icms_saida': 'sum',\n",
    "        'vl_icms_entrada': 'sum',\n",
    "        'vl_icms_cte': 'sum'\n",
    "    }).reset_index()\n",
    "\n",
    "    x = range(len(data_resumo))\n",
    "    width = 0.25\n",
    "    ax2.bar([i - width for i in x], data_resumo['vl_icms_saida'], width, label='DÃ©bito (SaÃ­da)', color='red', alpha=0.7)\n",
    "    ax2.bar(x, data_resumo['vl_icms_entrada'], width, label='CrÃ©dito (Entrada)', color='green', alpha=0.7)\n",
    "    ax2.bar([i + width for i in x], data_resumo['vl_icms_cte'], width, label='CrÃ©dito (CTe)', color='blue', alpha=0.7)\n",
    "    ax2.set_title(f'DÃ©bitos vs CrÃ©ditos - (Ãšltimos {len(periodos_plot)} PerÃ­odos)', fontweight='bold')\n",
    "    ax2.set_xlabel('PerÃ­odo')\n",
    "    ax2.set_ylabel('Valor (R$)')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(data_resumo['periodo'], rotation=45)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    # GrÃ¡fico 3: Receita Declarada vs Receita Apurada\n",
    "    ax3 = axes[1, 0]\n",
    "    data_receita = df_plot[df_plot['periodo'].isin(periodos_plot)].groupby('periodo').agg({\n",
    "        'bc_icms_saida': 'sum',\n",
    "        'receita_bruta_declarada': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    ax3.plot(data_receita['periodo'], data_receita['bc_icms_saida'], marker='o', label='BC ICMS (Apurada)', linewidth=2)\n",
    "    ax3.plot(data_receita['periodo'], data_receita['receita_bruta_declarada'], marker='s', label='Receita Declarada', linewidth=2)\n",
    "    ax3.fill_between(range(len(data_receita)), data_receita['bc_icms_saida'], data_receita['receita_bruta_declarada'], \n",
    "                      where=(data_receita['bc_icms_saida'] > data_receita['receita_bruta_declarada']), \n",
    "                      alpha=0.3, color='red', label='DiferenÃ§a (OmissÃ£o)')\n",
    "    ax3.set_title(f'Receita Declarada vs Apurada (Ãšltimos {len(periodos_plot)} PerÃ­odos)', fontweight='bold')\n",
    "    ax3.set_xlabel('PerÃ­odo')\n",
    "    ax3.set_ylabel('Valor (R$)')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "\n",
    "    # GrÃ¡fico 4: ComposiÃ§Ã£o do ICMS Devido\n",
    "    ax4 = axes[1, 1]\n",
    "    # Usa os totais do DataFrame (limitado)\n",
    "    totais = df_plot.sum() \n",
    "    componentes = ['DÃ©bito\\n(SaÃ­da)', 'Receita\\nOmitida', 'Declarado', 'CrÃ©dito\\n(Entrada)', 'CrÃ©dito\\n(CTe)']\n",
    "    valores = [\n",
    "        totais['vl_icms_saida'],\n",
    "        totais['vl_icms_receita_omitida'],\n",
    "        -totais['icms_declarado'],\n",
    "        -totais['vl_icms_entrada'],\n",
    "        -totais['vl_icms_cte']\n",
    "    ]\n",
    "    cores = ['red', 'orange', 'blue', 'green', 'cyan']\n",
    "    ax4.bar(componentes, valores, color=cores, alpha=0.7)\n",
    "    ax4.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    ax4.set_title('ComposiÃ§Ã£o do ICMS Devido (Total da Amostra)', fontweight='bold')\n",
    "    ax4.set_ylabel('Valor (R$)')\n",
    "    ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # âš  REMOVIDO: Bloco de estatÃ­sticas agora estÃ¡ na cÃ©lula anterior e Ã© calculado via Spark.\n",
    "    \n",
    "else:\n",
    "    print(\"âš  df_notif_pandas estÃ¡ vazio. Nenhum grÃ¡fico serÃ¡ gerado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a816da-adc4-479d-85fc-a69f4e14cafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# CÃ‰LULA 13: EXPORTAR RESULTADOS (âœ… PADRÃƒO OURO)\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXPORTANDO RESULTADOS PARA REDE LOCAL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import getpass\n",
    "import smbclient\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Configurar logging\n",
    "logging.getLogger('smbprotocol').setLevel(logging.WARNING)\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURAÃ‡ÃƒO DA CONEXÃƒO SMB\n",
    "# =============================================================================\n",
    "server = \"sef.sc.gov.br\"\n",
    "user = \"tsevero\" # âœ… Ajuste se necessÃ¡rio\n",
    "\n",
    "try:\n",
    "    pwd = getpass.getpass(f\"Digite a senha de rede para {user}@{server}: \")\n",
    "    smbclient.register_session(server, username=user, password=pwd)\n",
    "    print(f\"âœ“ SessÃ£o SMB registrada com sucesso para {user}!\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Falha ao registrar sessÃ£o SMB: {e}\")\n",
    "    raise\n",
    "\n",
    "# =============================================================================\n",
    "# DEFINIR DIRETÃ“RIO DE DESTINO\n",
    "# =============================================================================\n",
    "# Criar pasta com timestamp para organizar\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "dest_dir = r\"\\\\sef.sc.gov.br\\DFS\\GERFE08\\Backup Severo\\JUPYTER\\Analise_Fiscal_NFe_CTe\"\n",
    "dest_dir_timestamped = os.path.join(dest_dir, f\"GEI_{timestamp}\").replace(os.sep, '\\\\')\n",
    "\n",
    "print(f\"\\nCriando pasta de destino: {dest_dir_timestamped}\")\n",
    "try:\n",
    "    if not smbclient.path.isdir(dest_dir):\n",
    "        smbclient.makedirs(dest_dir)\n",
    "    smbclient.makedirs(dest_dir_timestamped)\n",
    "    print(\"âœ“ Pasta criada com sucesso\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Erro ao criar pasta: {e}\")\n",
    "    raise\n",
    "\n",
    "# =============================================================================\n",
    "# âœ… FUNÃ‡Ã•ES DE EXPORTAÃ‡ÃƒO \"PADRÃƒO OURO\"\n",
    "# =============================================================================\n",
    "\n",
    "# Limite mÃ¡ximo de linhas para um arquivo Excel\n",
    "LIMITE_EXPORT_EXCEL = 1048575 \n",
    "\n",
    "def salvar_pandas_na_rede(df_pandas, nome_arquivo, caminho_destino):\n",
    "    \"\"\"\n",
    "    Salva um DataFrame PANDAS (jÃ¡ coletado) em um arquivo Excel \n",
    "    passando por um temp local e copiando via SMB.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if len(df_pandas) == 0:\n",
    "            print(f\"  âš  DataFrame vazio, pulando: {nome_arquivo}\")\n",
    "            return False\n",
    "            \n",
    "        caminho_completo = os.path.join(caminho_destino, nome_arquivo).replace(os.sep, '\\\\')\n",
    "        temp_file = f\"/tmp/{nome_arquivo}\" # Arquivo temporÃ¡rio local no nÃ³ driver\n",
    "        \n",
    "        # Salvar como Excel localmente\n",
    "        print(f\"  ... salvando {len(df_pandas):,} linhas no temp local...\")\n",
    "        with pd.ExcelWriter(temp_file, engine='openpyxl') as writer:\n",
    "            df_pandas.to_excel(writer, index=False, sheet_name='Dados')\n",
    "        \n",
    "        # Copiar para a rede\n",
    "        print(f\"  ... copiando para {caminho_completo} ...\")\n",
    "        with open(temp_file, 'rb') as f_local:\n",
    "            with smbclient.open_file(caminho_completo, mode='wb') as f_remoto:\n",
    "                f_remoto.write(f_local.read())\n",
    "        \n",
    "        # Limpar arquivo temporÃ¡rio\n",
    "        os.remove(temp_file)\n",
    "        \n",
    "        print(f\"  âœ“ {nome_arquivo}: {len(df_pandas):,} registros exportados\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Erro ao salvar/copiar {nome_arquivo}: {e}\")\n",
    "        return False\n",
    "\n",
    "def exportar_view_spark_segura(view_name, order_by_cols, nome_arquivo, caminho_destino):\n",
    "    \"\"\"\n",
    "    Aplica a lÃ³gica de seguranÃ§a (Contar -> Decidir -> Limitar -> toPandas)\n",
    "    antes de chamar a funÃ§Ã£o de exportaÃ§Ã£o.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessando exportaÃ§Ã£o: {nome_arquivo} (View: {view_name})\")\n",
    "    \n",
    "    try:\n",
    "        df_spark = spark.table(view_name)\n",
    "        \n",
    "        # 1. CONTAR\n",
    "        print(f\"  ... contando registros em '{view_name}'...\")\n",
    "        total = df_spark.count()\n",
    "        \n",
    "        # 2. DECIDIR\n",
    "        if total == 0:\n",
    "            print(f\"  âš  View '{view_name}' estÃ¡ vazia. Pulando.\")\n",
    "            arquivos_erro.append(f\"{nome_arquivo} (Vazio)\")\n",
    "            return False\n",
    "        \n",
    "        df_para_exportar = df_spark\n",
    "        \n",
    "        if total > LIMITE_EXPORT_EXCEL:\n",
    "            print(f\"  âš  Muitos registros ({total:,}). Exportando APENAS os primeiros {LIMITE_EXPORT_EXCEL:,}...\")\n",
    "            # Ã‰ crucial ter um ORDER BY para o LIMIT ser determinÃ­stico\n",
    "            if order_by_cols:\n",
    "                # Usar Window Function para \"LIMIT\" determinÃ­stico antes de ordenar\n",
    "                # Isto Ã© mais robusto para performance do que OrderBy + Limit\n",
    "                window_spec = Window.orderBy(*order_by_cols)\n",
    "                df_para_exportar = (df_para_exportar\n",
    "                                    .withColumn(\"row_num\", row_number().over(window_spec))\n",
    "                                    .where(col(\"row_num\") <= LIMITE_EXPORT_EXCEL)\n",
    "                                    .drop(\"row_num\"))\n",
    "            else:\n",
    "                df_para_exportar = df_para_exportar.limit(LIMITE_EXPORT_EXCEL)\n",
    "        else:\n",
    "            print(f\"  âœ“ Coletando {total:,} registros para exportaÃ§Ã£o.\")\n",
    "            if order_by_cols:\n",
    "                df_para_exportar = df_para_exportar.orderBy(*order_by_cols)\n",
    "\n",
    "        # 3. toPandas (Agora Ã© seguro!)\n",
    "        print(\"  ... coletando dados para o Pandas...\")\n",
    "        df_pandas = df_para_exportar.toPandas()\n",
    "        \n",
    "        # 4. Exportar\n",
    "        if salvar_pandas_na_rede(df_pandas, nome_arquivo, caminho_destino):\n",
    "            arquivos_sucesso.append(nome_arquivo)\n",
    "            return True\n",
    "        else:\n",
    "            arquivos_erro.append(f\"{nome_arquivo} (Erro ao Salvar)\")\n",
    "            return False\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Erro GERAL ao processar '{view_name}': {e}\")\n",
    "        arquivos_erro.append(f\"{nome_arquivo} (Erro Spark: {e})\")\n",
    "        return False\n",
    "\n",
    "# =============================================================================\n",
    "# EXPORTAR ARQUIVOS PRINCIPAIS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"INICIANDO EXPORTAÃ‡ÃƒO DOS ARQUIVOS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "arquivos_sucesso = []\n",
    "arquivos_erro = []\n",
    "\n",
    "# Cachear as views que serÃ£o exportadas para acelerar os counts()\n",
    "print(\"Cacheando views para exportaÃ§Ã£o...\")\n",
    "views_para_exportar = [\n",
    "    \"vw_notificacao\", \"vw_entrada_destinatario\", \"vw_saida_emitente\", \n",
    "    \"vw_cte_filtrado\", \"vw_pgdas_filtrado\", \"vw_nfe_com_icms\"\n",
    "]\n",
    "for v in views_para_exportar:\n",
    "    try:\n",
    "        spark.table(v).cache()\n",
    "        print(f\"âœ“ View '{v}' cacheada.\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš  NÃ£o foi possÃ­vel cachear a view '{v}': {e}\")\n",
    "\n",
    "# 1. NOTIFICAÃ‡ÃƒO (ApuraÃ§Ã£o Mensal)\n",
    "exportar_view_spark_segura(\n",
    "    \"vw_notificacao\", \n",
    "    [\"cnpj\", \"periodo\"], \n",
    "    \"01_Notificacao.xlsx\", \n",
    "    dest_dir_timestamped\n",
    ")\n",
    "\n",
    "# 2. ENTRADA DESTINATÃRIO (CrÃ©ditos)\n",
    "exportar_view_spark_segura(\n",
    "    \"vw_entrada_destinatario\", \n",
    "    [\"periodo_ref\", \"chave_nfe\"], \n",
    "    \"02_Entrada_Destinatario.xlsx\", \n",
    "    dest_dir_timestamped\n",
    ")\n",
    "\n",
    "# 3. SAÃDA EMITENTE (DÃ©bitos)\n",
    "exportar_view_spark_segura(\n",
    "    \"vw_saida_emitente\", \n",
    "    [\"periodo_ref\", \"chave_nfe\"], \n",
    "    \"03_Saida_Emitente.xlsx\", \n",
    "    dest_dir_timestamped\n",
    ")\n",
    "\n",
    "# 4. CTe (CrÃ©dito de Frete)\n",
    "exportar_view_spark_segura(\n",
    "    \"vw_cte_filtrado\", \n",
    "    [\"periodo_ref\", \"chave_cte\"], \n",
    "    \"04_CTe.xlsx\", \n",
    "    dest_dir_timestamped\n",
    ")\n",
    "\n",
    "# 5. PGDAS-D (Simples Nacional)\n",
    "exportar_view_spark_segura(\n",
    "    \"vw_pgdas_filtrado\", \n",
    "    [\"cnpj\", \"periodo_ref\"], \n",
    "    \"05_PGDAS_D.xlsx\", \n",
    "    dest_dir_timestamped\n",
    ")\n",
    "\n",
    "# 6. RESUMO POR CNPJ (JÃ¡ calculado em Spark na CÃ©lula 11)\n",
    "print(\"\\nProcessando exportaÃ§Ã£o: 06_Resumo_por_CNPJ.xlsx\")\n",
    "try:\n",
    "    # Re-executar o resumo da CÃ©lula 11 para garantir\n",
    "    df_resumo_cnpj = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            cnpj,\n",
    "            COUNT(DISTINCT periodo) as qtde_periodos,\n",
    "            ROUND(SUM(vl_icms_saida), 2) as total_debitos,\n",
    "            ROUND(SUM(vl_icms_entrada), 2) as total_creditos_entrada,\n",
    "            ROUND(SUM(vl_icms_cte), 2) as total_creditos_cte,\n",
    "            ROUND(SUM(icms_declarado), 2) as total_declarado,\n",
    "            ROUND(SUM(vl_icms_devido), 2) as total_devido,\n",
    "            ROUND(SUM(receita_nao_declarada), 2) as total_receita_omitida\n",
    "        FROM vw_notificacao\n",
    "        GROUP BY cnpj\n",
    "        ORDER BY total_devido DESC\n",
    "    \"\"\")\n",
    "    df_pandas_resumo = df_resumo_cnpj.toPandas() # Seguro, pois Ã© agregado\n",
    "    \n",
    "    if salvar_pandas_na_rede(df_pandas_resumo, \"06_Resumo_por_CNPJ.xlsx\", dest_dir_timestamped):\n",
    "        arquivos_sucesso.append(\"06_Resumo_por_CNPJ.xlsx\")\n",
    "    else:\n",
    "        arquivos_erro.append(\"06_Resumo_por_CNPJ.xlsx (Erro ao Salvar)\")\n",
    "except Exception as e:\n",
    "    print(f\"  âœ— Erro GERAL ao processar '06_Resumo_por_CNPJ.xlsx': {e}\")\n",
    "    arquivos_erro.append(f\"06_Resumo_por_CNPJ.xlsx (Erro Spark: {e})\")\n",
    "\n",
    "# 7. TOP CFOPs\n",
    "print(\"\\nProcessando exportaÃ§Ã£o: 07_Top_CFOPs.xlsx\")\n",
    "try:\n",
    "    df_top_cfops = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            cfop, descricaocfop, entrada_saida, conta,\n",
    "            COUNT(*) as qtde_itens,\n",
    "            ROUND(SUM(valor_produto), 2) as valor_total\n",
    "        FROM vw_nfe_com_icms\n",
    "        GROUP BY cfop, descricaocfop, entrada_saida, conta\n",
    "        ORDER BY qtde_itens DESC\n",
    "        LIMIT 1000\n",
    "    \"\"\")\n",
    "    df_pandas_cfop = df_top_cfops.toPandas() # Seguro, pois Ã© limitado\n",
    "    \n",
    "    if salvar_pandas_na_rede(df_pandas_cfop, \"07_Top_CFOPs.xlsx\", dest_dir_timestamped):\n",
    "        arquivos_sucesso.append(\"07_Top_CFOPs.xlsx\")\n",
    "    else:\n",
    "        arquivos_erro.append(\"07_Top_CFOPs.xlsx (Erro ao Salvar)\")\n",
    "except Exception as e:\n",
    "    print(f\"  âœ— Erro GERAL ao processar '07_Top_CFOPs.xlsx': {e}\")\n",
    "    arquivos_erro.append(f\"07_Top_CFOPs.xlsx (Erro Spark: {e})\")\n",
    "\n",
    "# 8. TOP PRODUTOS\n",
    "print(\"\\nProcessando exportaÃ§Ã£o: 08_Top_Produtos.xlsx\")\n",
    "try:\n",
    "    df_top_produtos = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            descricao_produto, ncm,\n",
    "            COUNT(*) as qtde_vendas,\n",
    "            ROUND(SUM(valor_produto), 2) as valor_total\n",
    "        FROM vw_nfe_com_icms\n",
    "        WHERE entrada_saida = 'Saida' AND emitente_no_cadastro = 1\n",
    "        GROUP BY descricao_produto, ncm\n",
    "        ORDER BY valor_total DESC\n",
    "        LIMIT 1000\n",
    "    \"\"\")\n",
    "    df_pandas_produtos = df_top_produtos.toPandas() # Seguro, pois Ã© limitado\n",
    "    \n",
    "    if salvar_pandas_na_rede(df_pandas_produtos, \"08_Top_Produtos.xlsx\", dest_dir_timestamped):\n",
    "        arquivos_sucesso.append(\"08_Top_Produtos.xlsx\")\n",
    "    else:\n",
    "        arquivos_erro.append(\"08_Top_Produtos.xlsx (Erro ao Salvar)\")\n",
    "except Exception as e:\n",
    "    print(f\"  âœ— Erro GERAL ao processar '08_Top_Produtos.xlsx': {e}\")\n",
    "    arquivos_erro.append(f\"08_Top_Produtos.xlsx (Erro Spark: {e})\")\n",
    "    \n",
    "# =============================================================================\n",
    "# RELATÃ“RIO FINAL\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RELATÃ“RIO DE EXPORTAÃ‡ÃƒO\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nPasta de destino: {dest_dir_timestamped}\")\n",
    "print(f\"\\nArquivos exportados com sucesso: {len(arquivos_sucesso)}\")\n",
    "for arq in arquivos_sucesso:\n",
    "    print(f\"  âœ“ {arq}\")\n",
    "\n",
    "if arquivos_erro:\n",
    "    print(f\"\\nArquivos com erro ou vazios: {len(arquivos_erro)}\")\n",
    "    for arq in arquivos_erro:\n",
    "        print(f\"  âœ— {arq}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXPORTAÃ‡ÃƒO CONCLUÃDA!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Limpar todo o cache do Spark\n",
    "print(\"Limpando cache do Spark...\")\n",
    "spark.catalog.clearCache()\n",
    "print(\"âœ“ Cache limpo.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Data Pipeline)",
   "language": "python",
   "name": "conda_data_pipeline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
