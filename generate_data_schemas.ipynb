{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gerador de Data-Schemas - Projeto GEI\n",
    "\n",
    "Este notebook gera automaticamente a documenta√ß√£o de schema para todas as tabelas do projeto GEI.\n",
    "\n",
    "**Para cada tabela, ser√£o gerados 2 arquivos:**\n",
    "1. `{schema}__{tabela}__describe.txt` ‚Üí DESCRIBE FORMATTED\n",
    "2. `{schema}__{tabela}__sample.txt` ‚Üí SELECT * LIMIT 10\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Total de Tabelas:\n",
    "- **9 tabelas originais** (fontes externas)\n",
    "- **17 tabelas intermedi√°rias** (processadas pelo GEI)\n",
    "- **Total: 26 tabelas** ‚Üí 52 arquivos gerados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Configura√ß√£o do Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/tsevero/notebooks/SAT_BIG_DATA/data-pipeline/batch/poc\")\n",
    "sys.path.append(\"/home/tsevero/notebooks/SAT_BIG_DATA/data-pipeline/batch/plugins\")\n",
    "sys.path.append(\"/home/tsevero/notebooks/SAT_BIG_DATA/data-pipeline/batch/dags\")\n",
    "\n",
    "# Import libs python\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime\n",
    "\n",
    "# Import libs internas\n",
    "from utils import spark_utils_session as utils\n",
    "from hooks.hdfs.hdfs_helper import HdfsHelper\n",
    "from jobs.job_base_config import BaseETLJobClass\n",
    "\n",
    "import poc_helper\n",
    "poc_helper.load_env(\"PROD\")\n",
    "\n",
    "print(\"‚úÖ Ambiente configurado com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Inicializar Sess√£o Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session(profile: str, dynamic_allocation_enabled: bool = True) -> utils.DBASparkAppSession:\n",
    "    \"\"\"Gera DBASparkAppSession.\"\"\"\n",
    "    \n",
    "    app_name = \"gei_data_schema_generator\"\n",
    "    \n",
    "    spark_builder = (utils.DBASparkAppSession\n",
    "                     .builder\n",
    "                     .setAppName(app_name)\n",
    "                     .usingProcessProfile(profile)\n",
    "                    )\n",
    "    \n",
    "    if dynamic_allocation_enabled:\n",
    "        spark_builder.autoResourceManagement()\n",
    "\n",
    "    return spark_builder.build()\n",
    "\n",
    "session = get_session(profile='efd_t2')\n",
    "spark = session.sparkSession\n",
    "\n",
    "print(f\"‚úÖ Sess√£o Spark ('{spark.sparkContext.appName}') ativa e pronta para uso.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Defini√ß√£o das Tabelas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# =============================================================================\n",
    "# TABELAS ORIGINAIS (Fontes de Dados)\n",
    "# =============================================================================\n",
    "\n",
    "TABELAS_ORIGINAIS = [\n",
    "    (\"usr_sat_ods\", \"vw_ods_contrib\", \"Dados cadastrais de contribuintes (ODS)\"),\n",
    "    (\"usr_sat_ods\", \"vw_cad_vinculo\", \"V√≠nculos cadastrais (s√≥cios/respons√°veis)\"),\n",
    "    (\"usr_sat_ods\", \"sna_pgdasd_estabelecimento_raw\", \"Dados brutos PGDAS-D\"),\n",
    "    (\"nfe\", \"nfe\", \"Notas Fiscais Eletr√¥nicas\"),\n",
    "    (\"c115\", \"c115_dados_cadastrais_dest\", \"Conv√™nio 115\"),\n",
    "    (\"usr_sat_fsn\", \"fsn_conta_bancaria\", \"Contas banc√°rias\"),\n",
    "    (\"rais_caged\", \"vw_rais_vinculos\", \"V√≠nculos empregat√≠cios RAIS/CAGED\"),\n",
    "    (\"usr_sat_admcc\", \"acc_r66_totalestab\", \"Meios de pagamento\"),\n",
    "    (\"neaf\", \"empresa_indicio\", \"Ind√≠cios fiscais NEAF\"),\n",
    "]\n",
    "\n",
    "# =============================================================================\n",
    "# TABELAS INTERMEDI√ÅRIAS (Tabelas GEI)\n",
    "# =============================================================================\n",
    "\n",
    "TABELAS_INTERMEDIARIAS = [\n",
    "    # Principais\n",
    "    (\"gessimples\", \"gei_percent\", \"Tabela principal com scores e n√≠veis de risco\"),\n",
    "    (\"gessimples\", \"gei_cnpj\", \"Rela√ß√£o CNPJ ‚Üî Grupo Econ√¥mico\"),\n",
    "    (\"gessimples\", \"gei_cadastro\", \"Dados cadastrais consolidados\"),\n",
    "    (\"gessimples\", \"gei_contador\", \"Contadores dos grupos\"),\n",
    "    (\"gessimples\", \"gei_socios_compartilhados\", \"S√≥cios em m√∫ltiplas empresas\"),\n",
    "    (\"gessimples\", \"gei_c115_ranking_risco_grupo_economico\", \"Ranking de risco C115\"),\n",
    "    (\"gessimples\", \"gei_funcionarios_metricas_grupo\", \"M√©tricas RAIS/CAGED\"),\n",
    "    (\"gessimples\", \"gei_pagamentos_metricas_grupo\", \"M√©tricas de meios de pagamento\"),\n",
    "    (\"gessimples\", \"gei_c115_metricas_grupos\", \"M√©tricas C115 adicionais\"),\n",
    "    (\"gessimples\", \"gei_ccs_metricas_grupo\", \"M√©tricas de contas compartilhadas\"),\n",
    "    (\"gessimples\", \"gei_ccs_ranking_risco\", \"Ranking de risco CCS\"),\n",
    "\n",
    "    # Detalhadas CCS\n",
    "    (\"gessimples\", \"gei_ccs_cpf_compartilhado\", \"CPFs com contas em m√∫ltiplos CNPJs\"),\n",
    "    (\"gessimples\", \"gei_ccs_sobreposicao_responsaveis\", \"Respons√°veis com per√≠odos sobrepostos\"),\n",
    "    (\"gessimples\", \"gei_ccs_padroes_coordenados\", \"Eventos coordenados\"),\n",
    "\n",
    "    # Inconsist√™ncias\n",
    "    (\"gessimples\", \"gei_indicios\", \"Ind√≠cios fiscais catalogados\"),\n",
    "    (\"gessimples\", \"gei_nfe_completo\", \"NFe com inconsist√™ncias detectadas\"),\n",
    "    (\"gessimples\", \"gei_pgdas\", \"Dados PGDAS mensais\"),\n",
    "]\n",
    "\n",
    "print(f\"üìä Total de tabelas configuradas:\")\n",
    "print(f\"   - Originais: {len(TABELAS_ORIGINAIS)}\")\n",
    "print(f\"   - Intermedi√°rias: {len(TABELAS_INTERMEDIARIAS)}\")\n",
    "print(f\"   - TOTAL: {len(TABELAS_ORIGINAIS) + len(TABELAS_INTERMEDIARIAS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Fun√ß√µes de Processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criar_diretorios():\n",
    "    \"\"\"Cria estrutura de diret√≥rios para os data-schemas.\"\"\"\n",
    "    base_dir = Path(\"data-schemas\")\n",
    "    originais_dir = base_dir / \"originais\"\n",
    "    intermediarias_dir = base_dir / \"intermediarias\"\n",
    "\n",
    "    originais_dir.mkdir(parents=True, exist_ok=True)\n",
    "    intermediarias_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"‚úÖ Diret√≥rios criados:\")\n",
    "    print(f\"   - {originais_dir}\")\n",
    "    print(f\"   - {intermediarias_dir}\")\n",
    "\n",
    "    return originais_dir, intermediarias_dir\n",
    "\n",
    "\n",
    "def salvar_resultado(conteudo: str, caminho: Path):\n",
    "    \"\"\"Salva o resultado em um arquivo.\"\"\"\n",
    "    with open(caminho, 'w', encoding='utf-8') as f:\n",
    "        f.write(conteudo)\n",
    "    print(f\"   ‚úÖ Salvo: {caminho}\")\n",
    "\n",
    "\n",
    "def processar_tabela(spark, schema: str, tabela: str, descricao: str, diretorio: Path):\n",
    "    \"\"\"\n",
    "    Processa uma tabela: executa DESCRIBE FORMATTED e SELECT LIMIT 10.\n",
    "    Salva os resultados em arquivos separados.\n",
    "    \"\"\"\n",
    "    tabela_completa = f\"{schema}.{tabela}\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üîÑ Processando: {tabela_completa}\")\n",
    "    print(f\"   Descri√ß√£o: {descricao}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    # Nome base do arquivo\n",
    "    nome_arquivo_base = f\"{schema}__{tabela}\"\n",
    "\n",
    "    # =========================================================================\n",
    "    # 1. DESCRIBE FORMATTED\n",
    "    # =========================================================================\n",
    "    try:\n",
    "        print(f\"\\nüìã Executando DESCRIBE FORMATTED {tabela_completa}...\")\n",
    "        describe_df = spark.sql(f\"DESCRIBE FORMATTED {tabela_completa}\")\n",
    "\n",
    "        # Converte para string formatada\n",
    "        describe_output = []\n",
    "        describe_output.append(f\"# DESCRIBE FORMATTED: {tabela_completa}\\n\")\n",
    "        describe_output.append(f\"# Descri√ß√£o: {descricao}\\n\")\n",
    "        describe_output.append(f\"# Data/Hora: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        describe_output.append(\"\\n\" + \"=\"*80 + \"\\n\\n\")\n",
    "\n",
    "        # Coleta os dados\n",
    "        rows = describe_df.collect()\n",
    "        for row in rows:\n",
    "            linha = \" | \".join([str(col) if col is not None else \"\" for col in row])\n",
    "            describe_output.append(linha + \"\\n\")\n",
    "\n",
    "        # Salva o DESCRIBE FORMATTED\n",
    "        describe_path = diretorio / f\"{nome_arquivo_base}__describe.txt\"\n",
    "        salvar_resultado(''.join(describe_output), describe_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå ERRO ao executar DESCRIBE FORMATTED: {e}\")\n",
    "        describe_output = [f\"ERRO: {e}\\n\"]\n",
    "        describe_path = diretorio / f\"{nome_arquivo_base}__describe.txt\"\n",
    "        salvar_resultado(''.join(describe_output), describe_path)\n",
    "\n",
    "    # =========================================================================\n",
    "    # 2. SELECT * LIMIT 10\n",
    "    # =========================================================================\n",
    "    try:\n",
    "        print(f\"\\nüìä Executando SELECT * FROM {tabela_completa} LIMIT 10...\")\n",
    "        select_df = spark.sql(f\"SELECT * FROM {tabela_completa} LIMIT 10\")\n",
    "\n",
    "        # Converte para string formatada\n",
    "        select_output = []\n",
    "        select_output.append(f\"# SELECT * FROM {tabela_completa} LIMIT 10\\n\")\n",
    "        select_output.append(f\"# Descri√ß√£o: {descricao}\\n\")\n",
    "        select_output.append(f\"# Data/Hora: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        select_output.append(\"\\n\" + \"=\"*80 + \"\\n\\n\")\n",
    "\n",
    "        # Adiciona o schema\n",
    "        select_output.append(\"## SCHEMA:\\n\\n\")\n",
    "        for field in select_df.schema.fields:\n",
    "            select_output.append(f\"{field.name} | {field.dataType} | {field.nullable}\\n\")\n",
    "\n",
    "        select_output.append(\"\\n\" + \"=\"*80 + \"\\n\\n\")\n",
    "        select_output.append(\"## DADOS (primeiras 10 linhas):\\n\\n\")\n",
    "\n",
    "        # Coleta os dados em formato string\n",
    "        select_output.append(select_df._jdf.showString(10, 20, False))\n",
    "\n",
    "        # Salva o SELECT\n",
    "        select_path = diretorio / f\"{nome_arquivo_base}__sample.txt\"\n",
    "        salvar_resultado(''.join(select_output), select_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå ERRO ao executar SELECT: {e}\")\n",
    "        select_output = [f\"ERRO: {e}\\n\"]\n",
    "        select_path = diretorio / f\"{nome_arquivo_base}__sample.txt\"\n",
    "        salvar_resultado(''.join(select_output), select_path)\n",
    "\n",
    "    print(f\"\\n‚úÖ Tabela {tabela_completa} processada com sucesso!\")\n",
    "\n",
    "print(\"‚úÖ Fun√ß√µes de processamento definidas!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ EXECUTAR - Processar Todas as Tabelas\n",
    "\n",
    "**‚ö†Ô∏è ATEN√á√ÉO:** Esta c√©lula ir√° processar todas as 26 tabelas. O tempo estimado √© de **5-10 minutos** dependendo da performance do banco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GERADOR DE DATA-SCHEMAS - PROJETO GEI\")\n",
    "print(\"=\"*80)\n",
    "print(f\"In√≠cio: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Total de tabelas a processar: {len(TABELAS_ORIGINAIS) + len(TABELAS_INTERMEDIARIAS)}\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Cria estrutura de diret√≥rios\n",
    "dir_originais, dir_intermediarias = criar_diretorios()\n",
    "\n",
    "# Contadores\n",
    "total_sucesso = 0\n",
    "total_erro = 0\n",
    "\n",
    "# =============================================================================\n",
    "# Processar TABELAS ORIGINAIS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"üîµ\"*40)\n",
    "print(\"PROCESSANDO TABELAS ORIGINAIS (Fontes de Dados)\")\n",
    "print(\"üîµ\"*40 + \"\\n\")\n",
    "\n",
    "for schema, tabela, descricao in TABELAS_ORIGINAIS:\n",
    "    try:\n",
    "        processar_tabela(spark, schema, tabela, descricao, dir_originais)\n",
    "        total_sucesso += 1\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERRO CR√çTICO ao processar {schema}.{tabela}: {e}\")\n",
    "        total_erro += 1\n",
    "\n",
    "# =============================================================================\n",
    "# Processar TABELAS INTERMEDI√ÅRIAS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"üü¢\"*40)\n",
    "print(\"PROCESSANDO TABELAS INTERMEDI√ÅRIAS (Tabelas GEI)\")\n",
    "print(\"üü¢\"*40 + \"\\n\")\n",
    "\n",
    "for schema, tabela, descricao in TABELAS_INTERMEDIARIAS:\n",
    "    try:\n",
    "        processar_tabela(spark, schema, tabela, descricao, dir_intermediarias)\n",
    "        total_sucesso += 1\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERRO CR√çTICO ao processar {schema}.{tabela}: {e}\")\n",
    "        total_erro += 1\n",
    "\n",
    "# =============================================================================\n",
    "# RELAT√ìRIO FINAL\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RELAT√ìRIO FINAL\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úÖ Tabelas processadas com sucesso: {total_sucesso}\")\n",
    "print(f\"‚ùå Tabelas com erro: {total_erro}\")\n",
    "print(f\"üìÅ Arquivos salvos em: data-schemas/\")\n",
    "print(f\"   - Originais: {len(TABELAS_ORIGINAIS) * 2} arquivos\")\n",
    "print(f\"   - Intermedi√°rias: {len(TABELAS_INTERMEDIARIAS) * 2} arquivos\")\n",
    "print(f\"   - Total: {(len(TABELAS_ORIGINAIS) + len(TABELAS_INTERMEDIARIAS)) * 2} arquivos\")\n",
    "print(f\"\\nFim: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Verificar Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Lista todos os arquivos gerados\n",
    "print(\"üìÇ Arquivos gerados:\\n\")\n",
    "\n",
    "for root, dirs, files in os.walk(\"data-schemas\"):\n",
    "    level = root.replace(\"data-schemas\", \"\").count(os.sep)\n",
    "    indent = \" \" * 2 * level\n",
    "    print(f\"{indent}üìÅ {os.path.basename(root)}/\")\n",
    "    subindent = \" \" * 2 * (level + 1)\n",
    "    for file in sorted(files):\n",
    "        size = os.path.getsize(os.path.join(root, file))\n",
    "        print(f\"{subindent}üìÑ {file} ({size:,} bytes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ (OPCIONAL) Processar Tabela Individual\n",
    "\n",
    "Use esta c√©lula se quiser processar apenas uma tabela espec√≠fica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar aqui\n",
    "SCHEMA = \"gessimples\"\n",
    "TABELA = \"gei_percent\"\n",
    "DESCRICAO = \"Tabela principal com scores\"\n",
    "\n",
    "# Processar\n",
    "output_dir = Path(\"data-schemas/teste\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "processar_tabela(spark, SCHEMA, TABELA, DESCRICAO, output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
