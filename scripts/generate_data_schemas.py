"""
Script para gera√ß√£o autom√°tica de data-schemas do projeto GEI.
Gera DESCRIBE FORMATTED e SELECT * LIMIT 10 para todas as tabelas.

Autor: Sistema GEI
Data: 2025-11-17
"""

import sys
import os
from pathlib import Path
from datetime import datetime

# Adicionar paths necess√°rios (ajustar conforme seu ambiente)
# sys.path.append("/home/tsevero/notebooks/SAT_BIG_DATA/data-pipeline/batch/poc")
# sys.path.append("/home/tsevero/notebooks/SAT_BIG_DATA/data-pipeline/batch/plugins")
# sys.path.append("/home/tsevero/notebooks/SAT_BIG_DATA/data-pipeline/batch/dags")

# Import libs python
from pyspark.sql.types import *
from pyspark.sql.functions import *

# Import libs internas (descomentar se dispon√≠vel)
# from utils import spark_utils_session as utils
# import poc_helper
# poc_helper.load_env("PROD")


# =============================================================================
# CONFIGURA√á√ÉO DAS TABELAS
# =============================================================================

TABELAS_ORIGINAIS = [
    ("usr_sat_ods", "vw_ods_contrib", "Dados cadastrais de contribuintes (ODS)"),
    ("usr_sat_ods", "vw_cad_vinculo", "V√≠nculos cadastrais (s√≥cios/respons√°veis)"),
    ("usr_sat_ods", "sna_pgdasd_estabelecimento_raw", "Dados brutos PGDAS-D"),
    ("nfe", "nfe", "Notas Fiscais Eletr√¥nicas"),
    ("c115", "c115_dados_cadastrais_dest", "Conv√™nio 115"),
    ("usr_sat_fsn", "fsn_conta_bancaria", "Contas banc√°rias"),
    ("rais_caged", "vw_rais_vinculos", "V√≠nculos empregat√≠cios RAIS/CAGED"),
    ("usr_sat_admcc", "acc_r66_totalestab", "Meios de pagamento"),
    ("neaf", "empresa_indicio", "Ind√≠cios fiscais NEAF"),
]

TABELAS_INTERMEDIARIAS = [
    # Principais
    ("gessimples", "gei_percent", "Tabela principal com scores e n√≠veis de risco"),
    ("gessimples", "gei_cnpj", "Rela√ß√£o CNPJ ‚Üî Grupo Econ√¥mico"),
    ("gessimples", "gei_cadastro", "Dados cadastrais consolidados"),
    ("gessimples", "gei_contador", "Contadores dos grupos"),
    ("gessimples", "gei_socios_compartilhados", "S√≥cios em m√∫ltiplas empresas"),
    ("gessimples", "gei_c115_ranking_risco_grupo_economico", "Ranking de risco C115"),
    ("gessimples", "gei_funcionarios_metricas_grupo", "M√©tricas RAIS/CAGED"),
    ("gessimples", "gei_pagamentos_metricas_grupo", "M√©tricas de meios de pagamento"),
    ("gessimples", "gei_c115_metricas_grupos", "M√©tricas C115 adicionais"),
    ("gessimples", "gei_ccs_metricas_grupo", "M√©tricas de contas compartilhadas"),
    ("gessimples", "gei_ccs_ranking_risco", "Ranking de risco CCS"),

    # Detalhadas CCS
    ("gessimples", "gei_ccs_cpf_compartilhado", "CPFs com contas em m√∫ltiplos CNPJs"),
    ("gessimples", "gei_ccs_sobreposicao_responsaveis", "Respons√°veis com per√≠odos sobrepostos"),
    ("gessimples", "gei_ccs_padroes_coordenados", "Eventos coordenados"),

    # Inconsist√™ncias
    ("gessimples", "gei_indicios", "Ind√≠cios fiscais catalogados"),
    ("gessimples", "gei_nfe_completo", "NFe com inconsist√™ncias detectadas"),
    ("gessimples", "gei_pgdas", "Dados PGDAS mensais"),
]


# =============================================================================
# FUN√á√ïES AUXILIARES
# =============================================================================

def criar_diretorios():
    """Cria estrutura de diret√≥rios para os data-schemas."""
    base_dir = Path("data-schemas")
    originais_dir = base_dir / "originais"
    intermediarias_dir = base_dir / "intermediarias"

    originais_dir.mkdir(parents=True, exist_ok=True)
    intermediarias_dir.mkdir(parents=True, exist_ok=True)

    print(f"‚úÖ Diret√≥rios criados:")
    print(f"   - {originais_dir}")
    print(f"   - {intermediarias_dir}")

    return originais_dir, intermediarias_dir


def salvar_resultado(conteudo: str, caminho: Path):
    """Salva o resultado em um arquivo."""
    with open(caminho, 'w', encoding='utf-8') as f:
        f.write(conteudo)
    print(f"   ‚úÖ Salvo: {caminho}")


def processar_tabela(spark, schema: str, tabela: str, descricao: str, diretorio: Path):
    """
    Processa uma tabela: executa DESCRIBE FORMATTED e SELECT LIMIT 10.
    Salva os resultados em arquivos separados.
    """
    tabela_completa = f"{schema}.{tabela}"
    print(f"\n{'='*80}")
    print(f"üîÑ Processando: {tabela_completa}")
    print(f"   Descri√ß√£o: {descricao}")
    print(f"{'='*80}")

    # Nome base do arquivo
    nome_arquivo_base = f"{schema}__{tabela}"

    # =========================================================================
    # 1. DESCRIBE FORMATTED
    # =========================================================================
    try:
        print(f"\nüìã Executando DESCRIBE FORMATTED {tabela_completa}...")
        describe_df = spark.sql(f"DESCRIBE FORMATTED {tabela_completa}")

        # Converte para string formatada
        describe_output = []
        describe_output.append(f"# DESCRIBE FORMATTED: {tabela_completa}\n")
        describe_output.append(f"# Descri√ß√£o: {descricao}\n")
        describe_output.append(f"# Data/Hora: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        describe_output.append("\n" + "="*80 + "\n\n")

        # Coleta os dados
        rows = describe_df.collect()
        for row in rows:
            linha = " | ".join([str(col) if col is not None else "" for col in row])
            describe_output.append(linha + "\n")

        # Salva o DESCRIBE FORMATTED
        describe_path = diretorio / f"{nome_arquivo_base}__describe.txt"
        salvar_resultado(''.join(describe_output), describe_path)

    except Exception as e:
        print(f"   ‚ùå ERRO ao executar DESCRIBE FORMATTED: {e}")
        describe_output = [f"ERRO: {e}\n"]
        describe_path = diretorio / f"{nome_arquivo_base}__describe.txt"
        salvar_resultado(''.join(describe_output), describe_path)

    # =========================================================================
    # 2. SELECT * LIMIT 10
    # =========================================================================
    try:
        print(f"\nüìä Executando SELECT * FROM {tabela_completa} LIMIT 10...")
        select_df = spark.sql(f"SELECT * FROM {tabela_completa} LIMIT 10")

        # Converte para string formatada
        select_output = []
        select_output.append(f"# SELECT * FROM {tabela_completa} LIMIT 10\n")
        select_output.append(f"# Descri√ß√£o: {descricao}\n")
        select_output.append(f"# Data/Hora: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        select_output.append("\n" + "="*80 + "\n\n")

        # Adiciona o schema
        select_output.append("## SCHEMA:\n\n")
        for field in select_df.schema.fields:
            select_output.append(f"{field.name} | {field.dataType} | {field.nullable}\n")

        select_output.append("\n" + "="*80 + "\n\n")
        select_output.append("## DADOS (primeiras 10 linhas):\n\n")

        # Coleta os dados em formato string
        # Usa show() capturado como string
        select_output.append(select_df._jdf.showString(10, 20, False))

        # Salva o SELECT
        select_path = diretorio / f"{nome_arquivo_base}__sample.txt"
        salvar_resultado(''.join(select_output), select_path)

    except Exception as e:
        print(f"   ‚ùå ERRO ao executar SELECT: {e}")
        select_output = [f"ERRO: {e}\n"]
        select_path = diretorio / f"{nome_arquivo_base}__sample.txt"
        salvar_resultado(''.join(select_output), select_path)

    print(f"\n‚úÖ Tabela {tabela_completa} processada com sucesso!")


# =============================================================================
# FUN√á√ÉO PRINCIPAL
# =============================================================================

def main(spark):
    """Fun√ß√£o principal que processa todas as tabelas."""

    print("\n" + "="*80)
    print("GERADOR DE DATA-SCHEMAS - PROJETO GEI")
    print("="*80)
    print(f"In√≠cio: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"Total de tabelas a processar: {len(TABELAS_ORIGINAIS) + len(TABELAS_INTERMEDIARIAS)}")
    print("="*80 + "\n")

    # Cria estrutura de diret√≥rios
    dir_originais, dir_intermediarias = criar_diretorios()

    # Contadores
    total_sucesso = 0
    total_erro = 0

    # =========================================================================
    # Processar TABELAS ORIGINAIS
    # =========================================================================
    print("\n" + "üîµ"*40)
    print("PROCESSANDO TABELAS ORIGINAIS (Fontes de Dados)")
    print("üîµ"*40 + "\n")

    for schema, tabela, descricao in TABELAS_ORIGINAIS:
        try:
            processar_tabela(spark, schema, tabela, descricao, dir_originais)
            total_sucesso += 1
        except Exception as e:
            print(f"‚ùå ERRO CR√çTICO ao processar {schema}.{tabela}: {e}")
            total_erro += 1

    # =========================================================================
    # Processar TABELAS INTERMEDI√ÅRIAS
    # =========================================================================
    print("\n" + "üü¢"*40)
    print("PROCESSANDO TABELAS INTERMEDI√ÅRIAS (Tabelas GEI)")
    print("üü¢"*40 + "\n")

    for schema, tabela, descricao in TABELAS_INTERMEDIARIAS:
        try:
            processar_tabela(spark, schema, tabela, descricao, dir_intermediarias)
            total_sucesso += 1
        except Exception as e:
            print(f"‚ùå ERRO CR√çTICO ao processar {schema}.{tabela}: {e}")
            total_erro += 1

    # =========================================================================
    # RELAT√ìRIO FINAL
    # =========================================================================
    print("\n" + "="*80)
    print("RELAT√ìRIO FINAL")
    print("="*80)
    print(f"‚úÖ Tabelas processadas com sucesso: {total_sucesso}")
    print(f"‚ùå Tabelas com erro: {total_erro}")
    print(f"üìÅ Arquivos salvos em: data-schemas/")
    print(f"   - Originais: {len(TABELAS_ORIGINAIS) * 2} arquivos")
    print(f"   - Intermedi√°rias: {len(TABELAS_INTERMEDIARIAS) * 2} arquivos")
    print(f"   - Total: {(len(TABELAS_ORIGINAIS) + len(TABELAS_INTERMEDIARIAS)) * 2} arquivos")
    print(f"\nFim: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*80 + "\n")


# =============================================================================
# EXECU√á√ÉO
# =============================================================================

if __name__ == "__main__":
    print("""
    ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
    ‚ïë                  GERADOR DE DATA-SCHEMAS - GEI                         ‚ïë
    ‚ïë                                                                        ‚ïë
    ‚ïë  Este script gera automaticamente a documenta√ß√£o de schema para       ‚ïë
    ‚ïë  todas as tabelas do projeto GEI (originais e intermedi√°rias).        ‚ïë
    ‚ïë                                                                        ‚ïë
    ‚ïë  Para cada tabela, ser√£o gerados 2 arquivos:                          ‚ïë
    ‚ïë    1. {schema}__{tabela}__describe.txt (DESCRIBE FORMATTED)           ‚ïë
    ‚ïë    2. {schema}__{tabela}__sample.txt (SELECT * LIMIT 10)              ‚ïë
    ‚ïë                                                                        ‚ïë
    ‚ïë  IMPORTANTE: Execute este script em um ambiente com:                  ‚ïë
    ‚ïë    - Sess√£o Spark ativa                                               ‚ïë
    ‚ïë    - Acesso ao banco de dados Impala                                  ‚ïë
    ‚ïë    - Permiss√µes de leitura nas tabelas                                ‚ïë
    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """)

    # Verifica se spark est√° dispon√≠vel
    try:
        # Op√ß√£o 1: Se estiver em notebook Jupyter com sess√£o j√° criada
        if 'spark' in dir():
            print("‚úÖ Usando sess√£o Spark existente do notebook.")
            main(spark)

        # Op√ß√£o 2: Se tiver a fun√ß√£o get_session do notebook
        elif 'session' in dir():
            print("‚úÖ Usando session.sparkSession do notebook.")
            spark = session.sparkSession
            main(spark)

        # Op√ß√£o 3: Criar nova sess√£o (descomentar e ajustar)
        else:
            print("‚ö†Ô∏è  Nenhuma sess√£o Spark encontrada.")
            print("    Por favor, execute este script em um notebook Jupyter")
            print("    com uma sess√£o Spark j√° inicializada, ou descomente")
            print("    o c√≥digo abaixo para criar uma nova sess√£o.")
            print("\n    Exemplo de uso no notebook:")
            print("    >>> exec(open('scripts/generate_data_schemas.py').read())")

            # Descomentar para criar nova sess√£o
            # from utils import spark_utils_session as utils
            # def get_session(profile: str) -> utils.DBASparkAppSession:
            #     app_name = "gei_data_schema_generator"
            #     return (utils.DBASparkAppSession
            #            .builder
            #            .setAppName(app_name)
            #            .usingProcessProfile(profile)
            #            .autoResourceManagement()
            #            .build())
            #
            # session = get_session(profile='efd_t2')
            # spark = session.sparkSession
            # main(spark)

    except Exception as e:
        print(f"‚ùå ERRO CR√çTICO: {e}")
        print("\nPor favor, verifique:")
        print("  1. A sess√£o Spark est√° ativa?")
        print("  2. Voc√™ tem acesso ao banco de dados?")
        print("  3. As bibliotecas necess√°rias est√£o instaladas?")
